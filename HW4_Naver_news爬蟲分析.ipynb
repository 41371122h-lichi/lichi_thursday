{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/41371122h-lichi/lichi_thursday/blob/main/HW4_Naver_news%E7%88%AC%E8%9F%B2%E5%88%86%E6%9E%90.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UhVAbRkjRQr7"
      },
      "source": [
        "# Naver newsåˆ†æ (ä½œæ¥­å››)\n",
        "\n",
        "**ä¸»é¡Œç™¼æƒ³**\n",
        "\n",
        "å› ç‚ºæœ€è¿‘åœ¨æº–å‚™éŸ“æª¢(å·²è€ƒå®Œ)ï¼Œæ‰€ä»¥å¸¸å¸¸æœƒéœ€è¦é–±è®€æ–°èï¼Œå°±æƒ³èªªèƒ½ä¸èƒ½æŠŠNaverçš„æ–°èæ‹¿ä¾†åšåˆ†æï¼Œçœ‹çœ‹éŸ“åœ‹äººå¹³å¸¸éƒ½æ¥æ”¶åˆ°æ€éº¼æ¨£çš„è³‡è¨Šï¼Œä¹Ÿå¯ä»¥é€éç†±è©åˆ†æå¢åŠ èªè­˜çš„è©å½™é‡ï¼\n",
        "\n",
        "**åŠŸèƒ½ä»‹ç´¹**\n",
        "\n",
        "1.   çˆ¬èŸ²/ç†±è©åˆ†æ/AIç¸½çµä¸€æ­¥åˆ°ä½\n",
        "2.   ç´” Â· çˆ¬å–è³‡æ–™\n",
        "1.   ç´” Â· ç†±è©åˆ†æ\n",
        "2.   ç´” Â· AIç¸½çµ\n",
        "1.   åŒ¯å‡ºCSV/JSON(å¯é¸æ“‡å–®ä¸€/ä¸‰å¼µå·¥ä½œè¡¨åŒ¯å‡º)\n",
        "2.   å·¥ä½œè¡¨å‘½å/åˆªé™¤\n",
        "\n",
        "**å„é …åƒè€ƒURL**\n",
        "\n",
        "Â· [Naver news](https://news.naver.com/)\n",
        "\n",
        "Â· [Sheet URL](https://docs.google.com/spreadsheets/d/1P4V-D8o7bXHHHVMRQxG6voDjeyVUcDRB_PAahWajUUM/edit?usp=sharing)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 142,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WVm0_myQRNGZ",
        "outputId": "2a259c80-18c4-4bef-c5ff-9faa2557402d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (2.32.4)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.12/dist-packages (4.13.5)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests) (2025.10.5)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.12/dist-packages (from beautifulsoup4) (2.8)\n",
            "Requirement already satisfied: typing-extensions>=4.0.0 in /usr/local/lib/python3.12/dist-packages (from beautifulsoup4) (4.15.0)\n"
          ]
        }
      ],
      "source": [
        "pip install requests beautifulsoup4"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 143,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wPE-GAxTRV_6",
        "outputId": "1d509e8f-d067-410c-9f95-7e91dd0cf1da"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: konlpy in /usr/local/lib/python3.12/dist-packages (0.6.0)\n",
            "Requirement already satisfied: JPype1>=0.7.0 in /usr/local/lib/python3.12/dist-packages (from konlpy) (1.6.0)\n",
            "Requirement already satisfied: lxml>=4.1.0 in /usr/local/lib/python3.12/dist-packages (from konlpy) (5.4.0)\n",
            "Requirement already satisfied: numpy>=1.6 in /usr/local/lib/python3.12/dist-packages (from konlpy) (2.0.2)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.12/dist-packages (from JPype1>=0.7.0->konlpy) (25.0)\n"
          ]
        }
      ],
      "source": [
        "pip install konlpy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 144,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9pvdjdtCRXs2",
        "outputId": "83a41033-e9e6-47bc-d650-5cdc2fb56e10"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: gspread_dataframe in /usr/local/lib/python3.12/dist-packages (4.0.0)\n",
            "Requirement already satisfied: gspread>=3.0.0 in /usr/local/lib/python3.12/dist-packages (from gspread_dataframe) (6.2.1)\n",
            "Requirement already satisfied: pandas>=0.24.0 in /usr/local/lib/python3.12/dist-packages (from gspread_dataframe) (2.2.2)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.12/dist-packages (from gspread_dataframe) (1.17.0)\n",
            "Requirement already satisfied: google-auth>=1.12.0 in /usr/local/lib/python3.12/dist-packages (from gspread>=3.0.0->gspread_dataframe) (2.38.0)\n",
            "Requirement already satisfied: google-auth-oauthlib>=0.4.1 in /usr/local/lib/python3.12/dist-packages (from gspread>=3.0.0->gspread_dataframe) (1.2.2)\n",
            "Requirement already satisfied: numpy>=1.26.0 in /usr/local/lib/python3.12/dist-packages (from pandas>=0.24.0->gspread_dataframe) (2.0.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas>=0.24.0->gspread_dataframe) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas>=0.24.0->gspread_dataframe) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas>=0.24.0->gspread_dataframe) (2025.2)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.12/dist-packages (from google-auth>=1.12.0->gspread>=3.0.0->gspread_dataframe) (5.5.2)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.12/dist-packages (from google-auth>=1.12.0->gspread>=3.0.0->gspread_dataframe) (0.4.2)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.12/dist-packages (from google-auth>=1.12.0->gspread>=3.0.0->gspread_dataframe) (4.9.1)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.12/dist-packages (from google-auth-oauthlib>=0.4.1->gspread>=3.0.0->gspread_dataframe) (2.0.0)\n",
            "Requirement already satisfied: pyasn1<0.7.0,>=0.6.1 in /usr/local/lib/python3.12/dist-packages (from pyasn1-modules>=0.2.1->google-auth>=1.12.0->gspread>=3.0.0->gspread_dataframe) (0.6.1)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.12/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib>=0.4.1->gspread>=3.0.0->gspread_dataframe) (3.3.1)\n",
            "Requirement already satisfied: requests>=2.0.0 in /usr/local/lib/python3.12/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib>=0.4.1->gspread>=3.0.0->gspread_dataframe) (2.32.4)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests>=2.0.0->requests-oauthlib>=0.7.0->google-auth-oauthlib>=0.4.1->gspread>=3.0.0->gspread_dataframe) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests>=2.0.0->requests-oauthlib>=0.7.0->google-auth-oauthlib>=0.4.1->gspread>=3.0.0->gspread_dataframe) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests>=2.0.0->requests-oauthlib>=0.7.0->google-auth-oauthlib>=0.4.1->gspread>=3.0.0->gspread_dataframe) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests>=2.0.0->requests-oauthlib>=0.7.0->google-auth-oauthlib>=0.4.1->gspread>=3.0.0->gspread_dataframe) (2025.10.5)\n"
          ]
        }
      ],
      "source": [
        "pip install gspread_dataframe"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 145,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SondYDHQRZRg",
        "outputId": "8bd4b930-f2d3-433f-b71f-6060dea8c666"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: google-genai in /usr/local/lib/python3.12/dist-packages (1.45.0)\n",
            "Requirement already satisfied: anyio<5.0.0,>=4.8.0 in /usr/local/lib/python3.12/dist-packages (from google-genai) (4.11.0)\n",
            "Requirement already satisfied: google-auth<3.0.0,>=2.14.1 in /usr/local/lib/python3.12/dist-packages (from google-genai) (2.38.0)\n",
            "Requirement already satisfied: httpx<1.0.0,>=0.28.1 in /usr/local/lib/python3.12/dist-packages (from google-genai) (0.28.1)\n",
            "Requirement already satisfied: pydantic<3.0.0,>=2.0.0 in /usr/local/lib/python3.12/dist-packages (from google-genai) (2.11.10)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.28.1 in /usr/local/lib/python3.12/dist-packages (from google-genai) (2.32.4)\n",
            "Requirement already satisfied: tenacity<9.2.0,>=8.2.3 in /usr/local/lib/python3.12/dist-packages (from google-genai) (8.5.0)\n",
            "Requirement already satisfied: websockets<15.1.0,>=13.0.0 in /usr/local/lib/python3.12/dist-packages (from google-genai) (15.0.1)\n",
            "Requirement already satisfied: typing-extensions<5.0.0,>=4.11.0 in /usr/local/lib/python3.12/dist-packages (from google-genai) (4.15.0)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.12/dist-packages (from anyio<5.0.0,>=4.8.0->google-genai) (3.11)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.12/dist-packages (from anyio<5.0.0,>=4.8.0->google-genai) (1.3.1)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.12/dist-packages (from google-auth<3.0.0,>=2.14.1->google-genai) (5.5.2)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.12/dist-packages (from google-auth<3.0.0,>=2.14.1->google-genai) (0.4.2)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.12/dist-packages (from google-auth<3.0.0,>=2.14.1->google-genai) (4.9.1)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.12/dist-packages (from httpx<1.0.0,>=0.28.1->google-genai) (2025.10.5)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx<1.0.0,>=0.28.1->google-genai) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.12/dist-packages (from httpcore==1.*->httpx<1.0.0,>=0.28.1->google-genai) (0.16.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<3.0.0,>=2.0.0->google-genai) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.12/dist-packages (from pydantic<3.0.0,>=2.0.0->google-genai) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<3.0.0,>=2.0.0->google-genai) (0.4.2)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.28.1->google-genai) (3.4.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.28.1->google-genai) (2.5.0)\n",
            "Requirement already satisfied: pyasn1<0.7.0,>=0.6.1 in /usr/local/lib/python3.12/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3.0.0,>=2.14.1->google-genai) (0.6.1)\n"
          ]
        }
      ],
      "source": [
        "pip install google-genai"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 146,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z0Aur1oxRavI",
        "outputId": "44e4dd01-8853-4ffe-9c46-bf62cd47ad10"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: gradio in /usr/local/lib/python3.12/dist-packages (5.49.1)\n",
            "Requirement already satisfied: aiofiles<25.0,>=22.0 in /usr/local/lib/python3.12/dist-packages (from gradio) (24.1.0)\n",
            "Requirement already satisfied: anyio<5.0,>=3.0 in /usr/local/lib/python3.12/dist-packages (from gradio) (4.11.0)\n",
            "Requirement already satisfied: brotli>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from gradio) (1.1.0)\n",
            "Requirement already satisfied: fastapi<1.0,>=0.115.2 in /usr/local/lib/python3.12/dist-packages (from gradio) (0.119.0)\n",
            "Requirement already satisfied: ffmpy in /usr/local/lib/python3.12/dist-packages (from gradio) (0.6.3)\n",
            "Requirement already satisfied: gradio-client==1.13.3 in /usr/local/lib/python3.12/dist-packages (from gradio) (1.13.3)\n",
            "Requirement already satisfied: groovy~=0.1 in /usr/local/lib/python3.12/dist-packages (from gradio) (0.1.2)\n",
            "Requirement already satisfied: httpx<1.0,>=0.24.1 in /usr/local/lib/python3.12/dist-packages (from gradio) (0.28.1)\n",
            "Requirement already satisfied: huggingface-hub<2.0,>=0.33.5 in /usr/local/lib/python3.12/dist-packages (from gradio) (0.35.3)\n",
            "Requirement already satisfied: jinja2<4.0 in /usr/local/lib/python3.12/dist-packages (from gradio) (3.1.6)\n",
            "Requirement already satisfied: markupsafe<4.0,>=2.0 in /usr/local/lib/python3.12/dist-packages (from gradio) (3.0.3)\n",
            "Requirement already satisfied: numpy<3.0,>=1.0 in /usr/local/lib/python3.12/dist-packages (from gradio) (2.0.2)\n",
            "Requirement already satisfied: orjson~=3.0 in /usr/local/lib/python3.12/dist-packages (from gradio) (3.11.3)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.12/dist-packages (from gradio) (25.0)\n",
            "Requirement already satisfied: pandas<3.0,>=1.0 in /usr/local/lib/python3.12/dist-packages (from gradio) (2.2.2)\n",
            "Requirement already satisfied: pillow<12.0,>=8.0 in /usr/local/lib/python3.12/dist-packages (from gradio) (11.3.0)\n",
            "Requirement already satisfied: pydantic<2.12,>=2.0 in /usr/local/lib/python3.12/dist-packages (from gradio) (2.11.10)\n",
            "Requirement already satisfied: pydub in /usr/local/lib/python3.12/dist-packages (from gradio) (0.25.1)\n",
            "Requirement already satisfied: python-multipart>=0.0.18 in /usr/local/lib/python3.12/dist-packages (from gradio) (0.0.20)\n",
            "Requirement already satisfied: pyyaml<7.0,>=5.0 in /usr/local/lib/python3.12/dist-packages (from gradio) (6.0.3)\n",
            "Requirement already satisfied: ruff>=0.9.3 in /usr/local/lib/python3.12/dist-packages (from gradio) (0.14.1)\n",
            "Requirement already satisfied: safehttpx<0.2.0,>=0.1.6 in /usr/local/lib/python3.12/dist-packages (from gradio) (0.1.6)\n",
            "Requirement already satisfied: semantic-version~=2.0 in /usr/local/lib/python3.12/dist-packages (from gradio) (2.10.0)\n",
            "Requirement already satisfied: starlette<1.0,>=0.40.0 in /usr/local/lib/python3.12/dist-packages (from gradio) (0.48.0)\n",
            "Requirement already satisfied: tomlkit<0.14.0,>=0.12.0 in /usr/local/lib/python3.12/dist-packages (from gradio) (0.13.3)\n",
            "Requirement already satisfied: typer<1.0,>=0.12 in /usr/local/lib/python3.12/dist-packages (from gradio) (0.19.2)\n",
            "Requirement already satisfied: typing-extensions~=4.0 in /usr/local/lib/python3.12/dist-packages (from gradio) (4.15.0)\n",
            "Requirement already satisfied: uvicorn>=0.14.0 in /usr/local/lib/python3.12/dist-packages (from gradio) (0.37.0)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.12/dist-packages (from gradio-client==1.13.3->gradio) (2025.3.0)\n",
            "Requirement already satisfied: websockets<16.0,>=13.0 in /usr/local/lib/python3.12/dist-packages (from gradio-client==1.13.3->gradio) (15.0.1)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.12/dist-packages (from anyio<5.0,>=3.0->gradio) (3.11)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.12/dist-packages (from anyio<5.0,>=3.0->gradio) (1.3.1)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.12/dist-packages (from httpx<1.0,>=0.24.1->gradio) (2025.10.5)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx<1.0,>=0.24.1->gradio) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.12/dist-packages (from httpcore==1.*->httpx<1.0,>=0.24.1->gradio) (0.16.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<2.0,>=0.33.5->gradio) (3.20.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<2.0,>=0.33.5->gradio) (2.32.4)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<2.0,>=0.33.5->gradio) (4.67.1)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<2.0,>=0.33.5->gradio) (1.1.10)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas<3.0,>=1.0->gradio) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas<3.0,>=1.0->gradio) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas<3.0,>=1.0->gradio) (2025.2)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<2.12,>=2.0->gradio) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.12/dist-packages (from pydantic<2.12,>=2.0->gradio) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<2.12,>=2.0->gradio) (0.4.2)\n",
            "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.12/dist-packages (from typer<1.0,>=0.12->gradio) (8.3.0)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.12/dist-packages (from typer<1.0,>=0.12->gradio) (1.5.4)\n",
            "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.12/dist-packages (from typer<1.0,>=0.12->gradio) (13.9.4)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->pandas<3.0,>=1.0->gradio) (1.17.0)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.12/dist-packages (from rich>=10.11.0->typer<1.0,>=0.12->gradio) (4.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.12/dist-packages (from rich>=10.11.0->typer<1.0,>=0.12->gradio) (2.19.2)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface-hub<2.0,>=0.33.5->gradio) (3.4.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface-hub<2.0,>=0.33.5->gradio) (2.5.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.12/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0,>=0.12->gradio) (0.1.2)\n"
          ]
        }
      ],
      "source": [
        "pip install gradio"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 147,
      "metadata": {
        "id": "nflfR4R8RcOL"
      },
      "outputs": [],
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import pandas as pd\n",
        "import re\n",
        "from konlpy.tag import Okt\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "import time\n",
        "import tempfile\n",
        "import os\n",
        "import zipfile\n",
        "import shutil\n",
        "\n",
        "from google.colab import auth\n",
        "from google.auth import default\n",
        "import gspread\n",
        "from gspread_dataframe import set_with_dataframe\n",
        "import gspread.exceptions\n",
        "\n",
        "from google import genai\n",
        "from google.genai import types\n",
        "\n",
        "import gradio as gr\n",
        "\n",
        "okt = Okt()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 148,
      "metadata": {
        "id": "yzvWEaR7RfER"
      },
      "outputs": [],
      "source": [
        "try:\n",
        "    auth.authenticate_user()\n",
        "    creds, _ = default()\n",
        "    gc = gspread.authorize(creds)\n",
        "    sheets_enabled = True\n",
        "    sheet_auth_log = \"âœ… Google Sheets èªè­‰æˆåŠŸã€‚\"\n",
        "except Exception as e:\n",
        "    sheet_auth_log = f\"âŒ Google Sheets èªè­‰å¤±æ•—: {e}\"\n",
        "    sheets_enabled = False\n",
        "    gc = None\n",
        "\n",
        "# 2. Gemini API èªè­‰ (ä½¿ç”¨æ‚¨æª”æ¡ˆä¸­æä¾›çš„é‡‘é‘°å’Œæ¨¡å‹)\n",
        "GEMINI_API_KEY = \"AIzaSyBufFzIl5hJR-mPetOWfBxR7NnN8lPZHLM\"\n",
        "GEMINI_MODEL_NAME = \"gemini-2.5-flash\" # å»ºè­°ä½¿ç”¨ flash æ¨¡å‹ä»¥æé«˜é€Ÿåº¦\n",
        "\n",
        "try:\n",
        "    # è¨­ç½® API é‡‘é‘°\n",
        "    GEMINI_CLIENT = genai.Client(api_key=GEMINI_API_KEY)\n",
        "    GEMINI_MODEL = GEMINI_MODEL_NAME\n",
        "\n",
        "    gemini_auth_log = f\"âœ… Gemini API å®¢æˆ¶ç«¯åˆå§‹åŒ–æˆåŠŸï¼Œæ¨¡å‹è¨­å®šç‚º {GEMINI_MODEL}ã€‚\"\n",
        "    gemini_enabled = True\n",
        "except Exception as e:\n",
        "    gemini_auth_log = f\"âŒ Gemini API åˆå§‹åŒ–å¤±æ•— (è«‹ç¢ºèªé‡‘é‘°æ˜¯å¦æ­£ç¢º): {e}\"\n",
        "    GEMINI_CLIENT = None\n",
        "    gemini_enabled = False"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 149,
      "metadata": {
        "id": "owqT_EG4Rh08"
      },
      "outputs": [],
      "source": [
        "HEADERS = {\n",
        "    \"User-Agent\": (\n",
        "        \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) \"\n",
        "        \"AppleWebKit/537.36 (KHTML, like Gecko) \"\n",
        "        \"Chrome/114.0.0.0 Safari/537.36\"\n",
        "    )\n",
        "}\n",
        "\n",
        "SHEET_KEY = \"1P4V-D8o7bXHHHVMRQxG6voDjeyVUcDRB_PAahWajUUM\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 150,
      "metadata": {
        "id": "ncsfRzG9Rk2Z"
      },
      "outputs": [],
      "source": [
        "def å„²å­˜_è‡³_Google_Sheet(data_list, sheet_key, sheet_name=\"çˆ¬èŸ²çµæœ\"):\n",
        "    global gc, sheets_enabled\n",
        "\n",
        "    if not sheets_enabled:\n",
        "        return \"âŒ Google Sheets èªè­‰æœªæˆåŠŸæˆ–æ¬Šé™ä¸è¶³ï¼Œç„¡æ³•å¯«å…¥è³‡æ–™ã€‚\"\n",
        "\n",
        "    if not data_list:\n",
        "        return \"âŒ è³‡æ–™æ¸…å–®ç‚ºç©ºï¼Œæœªå¯«å…¥ Google Sheetã€‚\"\n",
        "\n",
        "    try:\n",
        "        sheet = gc.open_by_key(sheet_key)\n",
        "        df = pd.DataFrame(data_list)\n",
        "        try:\n",
        "            worksheet = sheet.worksheet(sheet_name)\n",
        "        except gspread.WorksheetNotFound:\n",
        "            worksheet = sheet.add_worksheet(title=sheet_name, rows=max(len(df) + 1, 100), cols=max(len(df.columns), 20))\n",
        "\n",
        "        set_with_dataframe(worksheet, df)\n",
        "\n",
        "        return f\"âœ… æˆåŠŸå¯«å…¥ **{len(data_list)}** ç­†è³‡æ–™è‡³ Google Sheetï¼\\nå·¥ä½œè¡¨åç¨±: **{sheet_name}**\"\n",
        "\n",
        "    except Exception as e:\n",
        "        return f\"âŒ å¯«å…¥ Google Sheet ç™¼ç”ŸéŒ¯èª¤ (è«‹æª¢æŸ¥æ¬Šé™): {e}\"\n",
        "\n",
        "def å„²å­˜_ç†±è©_è‡³_Google_Sheet(word_scores, sheet_key, sheet_name=\"ç†±è©åˆ†æçµæœ\"):\n",
        "    global gc, sheets_enabled\n",
        "\n",
        "    if not sheets_enabled or not word_scores:\n",
        "        return \"âŒ ç„¡æ³•å¯«å…¥ç†±è©è³‡æ–™ã€‚\"\n",
        "\n",
        "    try:\n",
        "        sheet = gc.open_by_key(sheet_key)\n",
        "        df = pd.DataFrame(word_scores, columns=['ç†±è©', 'ç¸½é—œéµæ€§åˆ†æ•¸'])\n",
        "        df['æ’å'] = df.index + 1\n",
        "        df = df[['æ’å', 'ç†±è©', 'ç¸½é—œéµæ€§åˆ†æ•¸']]\n",
        "        try:\n",
        "            worksheet = sheet.worksheet(sheet_name)\n",
        "        except gspread.WorksheetNotFound:\n",
        "            worksheet = sheet.add_worksheet(title=sheet_name, rows=max(len(df) + 1, 100), cols=max(len(df.columns), 20))\n",
        "\n",
        "        set_with_dataframe(worksheet, df)\n",
        "\n",
        "        return f\"âœ… æˆåŠŸå¯«å…¥ **{len(word_scores)}** ç­†ç†±è©è³‡æ–™è‡³ Google Sheetï¼\\nå·¥ä½œè¡¨åç¨±: **{sheet_name}**\"\n",
        "\n",
        "    except Exception as e:\n",
        "        return f\"âŒ å¯«å…¥ç†±è©è‡³ Google Sheet ç™¼ç”ŸéŒ¯èª¤ (è«‹æª¢æŸ¥æ¬Šé™): {e}\"\n",
        "\n",
        "def å„²å­˜_åˆ†æçµæœ_è‡³_Google_Sheet(analysis_dict, sheet_key, sheet_name):\n",
        "    global gc, sheets_enabled\n",
        "\n",
        "    if not sheets_enabled or not analysis_dict:\n",
        "        return \"âŒ ç„¡æ³•å¯«å…¥åˆ†æçµæœã€‚\"\n",
        "\n",
        "    try:\n",
        "        sheet = gc.open_by_key(sheet_key)\n",
        "        data = {\n",
        "            'åˆ†æä¸»é¡Œ': ['ç†±è©ç¸½é«”è¶¨å‹¢'],\n",
        "            'Gemini_æ´å¯Ÿæ‘˜è¦': [analysis_dict.get('æ´å¯Ÿæ‘˜è¦', '')],\n",
        "            'Gemini_çµè«–': [analysis_dict.get('çµè«–', '')]\n",
        "        }\n",
        "        df = pd.DataFrame(data)\n",
        "\n",
        "        try:\n",
        "            worksheet = sheet.worksheet(sheet_name)\n",
        "        except gspread.WorksheetNotFound:\n",
        "            worksheet = sheet.add_worksheet(title=sheet_name, rows=max(len(df) + 1, 10), cols=max(len(df.columns), 10))\n",
        "\n",
        "        set_with_dataframe(worksheet, df)\n",
        "\n",
        "        return f\"âœ… æˆåŠŸå¯«å…¥ Gemini åˆ†æçµæœè‡³ Google Sheetï¼\\nå·¥ä½œè¡¨åç¨±: **{sheet_name}**\"\n",
        "\n",
        "    except Exception as e:\n",
        "        return f\"âŒ å¯«å…¥åˆ†æçµæœè‡³ Google Sheet ç™¼ç”ŸéŒ¯èª¤ (è«‹æª¢æŸ¥æ¬Šé™)ï¼š{e}\"\n",
        "\n",
        "\n",
        "def å¾_Google_Sheet_è®€å–è³‡æ–™(sheet_key, sheet_name):\n",
        "    global gc, sheets_enabled\n",
        "    if not sheets_enabled:\n",
        "        return pd.DataFrame(), \"âŒ Google Sheets èªè­‰æœªæˆåŠŸæˆ–æ¬Šé™ä¸è¶³ï¼Œç„¡æ³•è®€å–è³‡æ–™ã€‚\"\n",
        "    try:\n",
        "        sheet = gc.open_by_key(sheet_key)\n",
        "        worksheet = sheet.worksheet(sheet_name)\n",
        "        data = worksheet.get_all_records()\n",
        "        df = pd.DataFrame(data)\n",
        "        log = f\"âœ… æˆåŠŸè®€å– **{len(df)}** ç­†è³‡æ–™ã€‚\\n\"\n",
        "        return df, log\n",
        "    except gspread.exceptions.WorksheetNotFound:\n",
        "        return pd.DataFrame(), f\"âŒ è®€å– Google Sheet ç™¼ç”ŸéŒ¯èª¤ï¼šæ‰¾ä¸åˆ°å·¥ä½œè¡¨ **'{sheet_name}'**ï¼Œè«‹ç¢ºèªåç¨±æ˜¯å¦æ­£ç¢ºã€‚\"\n",
        "    except Exception as e:\n",
        "        return pd.DataFrame(), f\"âŒ è®€å– Google Sheet ç™¼ç”ŸéŒ¯èª¤ï¼š{e}\""
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def delete_sheets_by_suffix(sheet_suffix):\n",
        "    \"\"\"æ ¹æ“šå¾Œç¶´ï¼Œåˆªé™¤ç›¸é—œçš„ä¸‰å€‹å·¥ä½œè¡¨\"\"\"\n",
        "    global gc, sheets_enabled\n",
        "    full_log = \"--- èªè­‰ç‹€æ…‹ ---\\n\" + sheet_auth_log + \"\\n\" + \"=\" * 20 + \"\\n\"\n",
        "\n",
        "    if not sheets_enabled:\n",
        "        return full_log + \"\\nâŒ Google Sheets èªè­‰æœªæˆåŠŸï¼Œç„¡æ³•åˆªé™¤å·¥ä½œè¡¨ã€‚\"\n",
        "\n",
        "    if not sheet_suffix:\n",
        "        return full_log + \"\\nâŒ éŒ¯èª¤ï¼šè«‹è¼¸å…¥æœ‰æ•ˆçš„**å·¥ä½œè¡¨å¾Œç¶´**ä¾†æŒ‡å®šè¦åˆªé™¤çš„é›†åˆã€‚\"\n",
        "\n",
        "    RESULT_SHEET_NAME, TOP_WORDS_SHEET_NAME, AI_SUMMARY_SHEET_NAME = get_suffix_names(sheet_suffix)\n",
        "\n",
        "    target_sheet_names = [RESULT_SHEET_NAME, TOP_WORDS_SHEET_NAME, AI_SUMMARY_SHEET_NAME]\n",
        "\n",
        "    delete_count = 0\n",
        "\n",
        "    try:\n",
        "        sheet = gc.open_by_key(SHEET_KEY)\n",
        "        full_log += f\"ğŸ’¡ æ­£åœ¨å˜—è©¦åˆªé™¤èˆ‡å¾Œç¶´ **'{sheet_suffix}'** ç›¸é—œçš„å·¥ä½œè¡¨ï¼š\\n\"\n",
        "\n",
        "        for sheet_name in target_sheet_names:\n",
        "            try:\n",
        "                worksheet = sheet.worksheet(sheet_name)\n",
        "                sheet.del_worksheet(worksheet)\n",
        "                full_log += f\"âœ… æˆåŠŸåˆªé™¤å·¥ä½œè¡¨: **{sheet_name}**\\n\"\n",
        "                delete_count += 1\n",
        "            except gspread.WorksheetNotFound:\n",
        "                full_log += f\"âš  è­¦å‘Šï¼šæ‰¾ä¸åˆ°å·¥ä½œè¡¨: **{sheet_name}** (è·³é)\\n\"\n",
        "\n",
        "        if delete_count == 0:\n",
        "            full_log += \"\\nâŒ åˆªé™¤å®Œæˆï¼Œä½†æœªæ‰¾åˆ°ä»»ä½•ç›¸é—œå·¥ä½œè¡¨å¯ä¾›åˆªé™¤ã€‚\"\n",
        "        else:\n",
        "            full_log += f\"\\nâœ… **æˆåŠŸåˆªé™¤ {delete_count} å€‹å·¥ä½œè¡¨ï¼**\"\n",
        "\n",
        "    except Exception as e:\n",
        "        full_log += f\"\\nâŒ åˆªé™¤éç¨‹ä¸­ç™¼ç”ŸéŒ¯èª¤: {e}\"\n",
        "\n",
        "    return full_log"
      ],
      "metadata": {
        "id": "pGzEBbxDDjBg"
      },
      "execution_count": 151,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def export_sheet_to_csv_json(sheet_name, format_choice):\n",
        "    \"\"\"è®€å–æŒ‡å®šå·¥ä½œè¡¨ä¸¦åŒ¯å‡ºç‚º CSV æˆ– JSON æª”æ¡ˆ\"\"\"\n",
        "    global gc, sheets_enabled\n",
        "\n",
        "    if not sheets_enabled:\n",
        "        return None, \"âŒ Google Sheets èªè­‰æœªæˆåŠŸï¼Œç„¡æ³•è®€å–å·¥ä½œè¡¨ã€‚\"\n",
        "\n",
        "    if not sheet_name:\n",
        "        return None, \"âŒ éŒ¯èª¤ï¼šè«‹è¼¸å…¥è¦åŒ¯å‡ºçš„å·¥ä½œè¡¨åç¨±ã€‚\"\n",
        "\n",
        "    df, read_log = å¾_Google_Sheet_è®€å–è³‡æ–™(SHEET_KEY, sheet_name)\n",
        "\n",
        "    if df.empty:\n",
        "        return None, read_log + \"\\nâŒ è³‡æ–™ç‚ºç©ºæˆ–è®€å–å¤±æ•—ï¼Œç„¡æ³•åŒ¯å‡ºã€‚\"\n",
        "\n",
        "    # å‰µå»ºä¸€å€‹è‡¨æ™‚æ–‡ä»¶ä¾†å„²å­˜åŒ¯å‡ºçš„å…§å®¹\n",
        "    with tempfile.NamedTemporaryFile(mode='w', delete=False, encoding='utf-8') as tmp_file:\n",
        "        file_path = tmp_file.name\n",
        "\n",
        "        if format_choice == 'CSV':\n",
        "            # å°‡ DataFrame å¯«å…¥ CSVï¼Œä¸åŒ…å«ç´¢å¼•ï¼Œç¢ºä¿ç·¨ç¢¼\n",
        "            df.to_csv(file_path, index=False, encoding='utf-8')\n",
        "            output_log = f\"âœ… æˆåŠŸå°‡ **{sheet_name}** ({len(df)} ç­†è³‡æ–™) åŒ¯å‡ºç‚º CSV æª”æ¡ˆã€‚\"\n",
        "        elif format_choice == 'JSON':\n",
        "            # å°‡ DataFrame å¯«å…¥ JSON (orient='records' æ ¼å¼ç‚º [{...}, {...}])\n",
        "            df.to_json(file_path, orient='records', force_ascii=False, indent=4)\n",
        "            output_log = f\"âœ… æˆåŠŸå°‡ **{sheet_name}** ({len(df)} ç­†è³‡æ–™) åŒ¯å‡ºç‚º JSON æª”æ¡ˆã€‚\"\n",
        "        else:\n",
        "             return None, \"âŒ éŒ¯èª¤ï¼šç„¡æ•ˆçš„åŒ¯å‡ºæ ¼å¼é¸æ“‡ã€‚\"\n",
        "\n",
        "    # Gradio æœƒå°‡æ­¤æ–‡ä»¶è·¯å¾‘ä½œç‚ºä¸‹è¼‰é€£çµæä¾›\n",
        "    return file_path, output_log"
      ],
      "metadata": {
        "id": "LAb-HFv8Djpc"
      },
      "execution_count": 152,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def run_and_clear_single_export(sheet_name, format_choice):\n",
        "    \"\"\"\n",
        "    åŒ…è£å–®ä¸€å·¥ä½œè¡¨åŒ¯å‡ºå‡½æ•¸ï¼Œä¸¦åœ¨å®Œæˆå¾Œæ¸…ç©ºè¼¸å…¥æ¡†ã€‚\n",
        "    \"\"\"\n",
        "    file_path, output_log = export_sheet_to_csv_json(sheet_name, format_choice)\n",
        "\n",
        "    # å¿…é ˆä¾åºå›å‚³çµ¦ outputs åˆ—è¡¨ï¼š\n",
        "    # (1) ä¸‹è¼‰æª”æ¡ˆ, (2) Log è¨Šæ¯, (3) æ¸…ç©ºè¼¸å…¥æ¡†çš„å€¼\n",
        "    return file_path, output_log, \"\""
      ],
      "metadata": {
        "id": "U1RQs_fvJBWs"
      },
      "execution_count": 153,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def export_all_by_suffix(sheet_suffix, format_choice):\n",
        "    \"\"\"æ ¹æ“šå¾Œç¶´ï¼Œå°‡ä¸‰å€‹ç›¸é—œå·¥ä½œè¡¨è®€å–ä¸¦å£“ç¸®æˆå–®ä¸€ ZIP æª”æ¡ˆä¾›ä¸‹è¼‰ã€‚\"\"\"\n",
        "    global gc, sheets_enabled\n",
        "\n",
        "    if not sheets_enabled:\n",
        "        return None, \"âŒ Google Sheets èªè­‰æœªæˆåŠŸï¼Œç„¡æ³•è®€å–å·¥ä½œè¡¨ã€‚\"\n",
        "\n",
        "    if not sheet_suffix:\n",
        "        return None, \"âŒ éŒ¯èª¤ï¼šè«‹è¼¸å…¥è¦åŒ¯å‡ºçš„**å·¥ä½œè¡¨å¾Œç¶´**ã€‚\"\n",
        "\n",
        "    # å–å¾—ä¸‰å€‹å·¥ä½œè¡¨åç¨±\n",
        "    RESULT_SHEET_NAME, TOP_WORDS_SHEET_NAME, AI_SUMMARY_SHEET_NAME = get_suffix_names(sheet_suffix)\n",
        "    target_sheet_names = {\n",
        "        'åŸå§‹æ•¸æ“š': RESULT_SHEET_NAME,\n",
        "        'ç†±è©åˆ†æ': TOP_WORDS_SHEET_NAME,\n",
        "        'AIç¸½çµ': AI_SUMMARY_SHEET_NAME\n",
        "    }\n",
        "\n",
        "    # å‰µå»ºä¸€å€‹è‡¨æ™‚ç›®éŒ„ä¾†å­˜æ”¾ CSV/JSON æª”æ¡ˆ\n",
        "    temp_dir = tempfile.mkdtemp()\n",
        "\n",
        "    exported_files_count = 0\n",
        "    log_messages = []\n",
        "\n",
        "    try:\n",
        "        sheet = gc.open_by_key(SHEET_KEY)\n",
        "\n",
        "        for file_type, sheet_name in target_sheet_names.items():\n",
        "            try:\n",
        "                # 1. è®€å–è³‡æ–™\n",
        "                worksheet = sheet.worksheet(sheet_name)\n",
        "                data = worksheet.get_all_records()\n",
        "                df = pd.DataFrame(data)\n",
        "\n",
        "                if df.empty:\n",
        "                    log_messages.append(f\"âš  è­¦å‘Šï¼šå·¥ä½œè¡¨ **{sheet_name}** ({file_type}) ç‚ºç©ºï¼Œè·³éåŒ¯å‡ºã€‚\")\n",
        "                    continue\n",
        "\n",
        "                # 2. å¯«å…¥æš«å­˜æª”æ¡ˆ\n",
        "                ext = 'csv' if format_choice == 'CSV' else 'json'\n",
        "                base_file_name = f\"{sheet_name}.{ext}\"\n",
        "                file_path = os.path.join(temp_dir, base_file_name)\n",
        "\n",
        "                if format_choice == 'CSV':\n",
        "                    df.to_csv(file_path, index=False, encoding='utf-8')\n",
        "                elif format_choice == 'JSON':\n",
        "                    df.to_json(file_path, orient='records', force_ascii=False, indent=4)\n",
        "\n",
        "                exported_files_count += 1\n",
        "                log_messages.append(f\"âœ… æˆåŠŸè™•ç†å·¥ä½œè¡¨ **{sheet_name}** ({len(df)} ç­†è³‡æ–™)ã€‚\")\n",
        "\n",
        "            except gspread.exceptions.WorksheetNotFound:\n",
        "                log_messages.append(f\"âš  è­¦å‘Šï¼šæ‰¾ä¸åˆ°å·¥ä½œè¡¨ **{sheet_name}**ï¼Œè·³éåŒ¯å‡ºã€‚\")\n",
        "            except Exception as e:\n",
        "                log_messages.append(f\"âŒ åŒ¯å‡ºå·¥ä½œè¡¨ **{sheet_name}** æ™‚ç™¼ç”ŸéŒ¯èª¤: {e}\")\n",
        "\n",
        "        # 3. å‰µå»º ZIP å£“ç¸®æª”æ¡ˆ\n",
        "        if exported_files_count == 0:\n",
        "            return None, \"\\n\".join(log_messages) + \"\\nâŒ æ²’æœ‰æ‰¾åˆ°ä»»ä½•å·¥ä½œè¡¨å¯ä¾›åŒ¯å‡ºï¼Œæ“ä½œå¤±æ•—ã€‚\"\n",
        "\n",
        "        zip_filename = f\"Naver_Analysis_Export_{sheet_suffix}.zip\"\n",
        "        # ä½¿ç”¨ tempfile.gettempdir() ç¢ºä¿ ZIP æª”æ¡ˆåœ¨å¯å­˜å–çš„ä½ç½®\n",
        "        zip_path = os.path.join(tempfile.gettempdir(), zip_filename)\n",
        "\n",
        "        with zipfile.ZipFile(zip_path, 'w', zipfile.ZIP_DEFLATED) as zipf:\n",
        "            # éæ­·è‡¨æ™‚ç›®éŒ„ä¸­çš„æ‰€æœ‰æª”æ¡ˆä¸¦åŠ å…¥ ZIP\n",
        "            for file_name in os.listdir(temp_dir):\n",
        "                 file_path = os.path.join(temp_dir, file_name)\n",
        "                 zipf.write(file_path, file_name) # ç¬¬äºŒå€‹åƒæ•¸æ˜¯ ZIP å…§éƒ¨çš„æª”æ¡ˆåç¨±\n",
        "\n",
        "        final_log = \"\\n\".join(log_messages)\n",
        "        final_log += f\"\\n\\nâœ… **æ“ä½œå®Œæˆï¼** æˆåŠŸå°‡ {exported_files_count} å€‹æª”æ¡ˆæ‰“åŒ…æˆ ZIP ä¾›ä¸‹è¼‰ã€‚\"\n",
        "\n",
        "        return zip_path, final_log\n",
        "\n",
        "    except Exception as e:\n",
        "        return None, f\"âŒ åŒ¯å‡ºéç¨‹ä¸­ç™¼ç”Ÿåš´é‡éŒ¯èª¤: {e}\"\n",
        "\n",
        "    finally:\n",
        "        # ç„¡è«–æˆåŠŸæˆ–å¤±æ•—ï¼Œéƒ½è¦æ¸…ç†è‡¨æ™‚ç›®éŒ„\n",
        "        shutil.rmtree(temp_dir)"
      ],
      "metadata": {
        "id": "-9ijepX-Fbee"
      },
      "execution_count": 154,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 155,
      "metadata": {
        "id": "qRCDCtHtRo2b"
      },
      "outputs": [],
      "source": [
        "def çˆ¬å–_æ–‡ç« å…§æ–‡(article_url):\n",
        "    # å‡½æ•¸å…§å®¹åŒå‰\n",
        "    if not article_url:\n",
        "        return \"ç„¡é€£çµ\", \"ç„¡é€£çµ\", \"ç„¡å…§æ–‡\"\n",
        "    try:\n",
        "        response = requests.get(article_url, headers=HEADERS, timeout=10)\n",
        "        response.raise_for_status()\n",
        "        soup = BeautifulSoup(response.text, \"html.parser\")\n",
        "        content_tag = soup.select_one(\"#dic_area, .news_content, #articeBody, .article_body, .story_view .ct_area\")\n",
        "        if content_tag:\n",
        "          for tag in content_tag.find_all(['script', 'a', 'img', 'br', 'em', 'strong', 'iframe', 'figcaption']):\n",
        "              tag.decompose()\n",
        "          content = content_tag.get_text(strip=True)\n",
        "        else:\n",
        "          content = \"ç„¡å…§æ–‡\"\n",
        "        author_tag = soup.select_one(\"em.media_end_head_journalist_name, .media_end_head_journalist_name, .byline .journalist_name\")\n",
        "        author = author_tag.get_text(strip=True) if author_tag else \"ç„¡ä½œè€…\"\n",
        "        date_tag = soup.select_one(\"span.media_end_head_info_datestamp_time._ARTICLE_DATE_TIME, .info span.date\")\n",
        "        date = date_tag['data-date-time'] if date_tag and 'data-date-time' in date_tag.attrs else (\n",
        "            date_tag.get_text(strip=True) if date_tag else \"ç„¡æ—¥æœŸ\"\n",
        "        )\n",
        "        date = date.replace('ì…ë ¥ ', '').strip()\n",
        "        return date, author, content\n",
        "    except Exception as e:\n",
        "        return f\"å…§æ–‡é çˆ¬å–å¤±æ•—: {type(e).__name__}\", \"å…§æ–‡é çˆ¬å–å¤±æ•—\", \"ç„¡å…§æ–‡\"\n",
        "\n",
        "def çˆ¬å–_naver_æ–°è(url):\n",
        "    # å‡½æ•¸å…§å®¹åŒå‰\n",
        "    try:\n",
        "        response = requests.get(url, headers=HEADERS)\n",
        "        response.raise_for_status()\n",
        "    except Exception as e:\n",
        "        return [], f\"âŒ ä¸»åˆ—è¡¨çˆ¬å–å¤±æ•—ï¼šè«‹æª¢æŸ¥ç¶²å€æˆ–é€£ç·šå•é¡Œã€‚\\néŒ¯èª¤ï¼š{e}\"\n",
        "\n",
        "    soup = BeautifulSoup(response.text, \"html.parser\")\n",
        "    news_list = []\n",
        "\n",
        "    # åˆ—è¡¨ä¸»é¸æ“‡å™¨\n",
        "    news_items = soup.select(\"li.press_edit_news_item, li.sa_item, div.cluster_item\")\n",
        "    MAX_ITEMS = 20\n",
        "    news_items = news_items[:MAX_ITEMS]\n",
        "\n",
        "    if not news_items:\n",
        "        return [], \"âš  è­¦å‘Šï¼šåˆ—è¡¨é¸æ“‡å™¨å¯èƒ½ä¸åŒ¹é…ï¼Œç„¡æ³•æŠ“å–åˆ—è¡¨ã€‚è«‹æª¢æŸ¥ç¶²å€æ˜¯å¦ç‚ºæœ‰æ•ˆçš„ Naver åª’é«”æˆ–ä¸»é¡Œé ã€‚\"\n",
        "\n",
        "    current_log = f\"ğŸ’¡ ç¸½å…±æ‰¾åˆ° {len(news_items)} ç¯‡æ–°èï¼Œé–‹å§‹é€²è¡ŒäºŒæ¬¡çˆ¬å–...\\n\"\n",
        "\n",
        "    for n, item in enumerate(news_items, start=1):\n",
        "        # æŠ“å–æ¨™é¡Œ\n",
        "        title_tag = item.select_one(\"span.press_edit_news_title, a.sa_text_title\")\n",
        "        title = title_tag.get_text(strip=True) if title_tag else \"ç„¡æ¨™é¡Œ\"\n",
        "\n",
        "        # æŠ“å–æ–‡ç« é€£çµ\n",
        "        link_tag = item.select_one(\"a.press_edit_news_link, a.sa_text_title, a.n_tit\")\n",
        "        news_url = link_tag['href'] if link_tag and 'href' in link_tag.attrs else None\n",
        "\n",
        "        if not news_url or not news_url.startswith('https://n.news.naver.com'):\n",
        "             current_log += f\"[{n}/{MAX_ITEMS}] è·³éï¼šé€£çµç„¡æ•ˆæˆ–é Naver å…§é  ({news_url})\\n\"\n",
        "             continue\n",
        "\n",
        "        # å‘¼å«äºŒæ¬¡çˆ¬èŸ²\n",
        "        date, author, content = çˆ¬å–_æ–‡ç« å…§æ–‡(news_url)\n",
        "\n",
        "        current_log += f\"[{n}/{MAX_ITEMS}] è™•ç†ï¼š{title[:20]}... | å…§æ–‡é•·åº¦: {len(content)} å­—ç¬¦\\n\"\n",
        "\n",
        "        news_list.append({\n",
        "            \"æ¨™é¡Œ\": title,\n",
        "            \"æ—¥æœŸ\": date,\n",
        "            \"ä½œè€…\": author,\n",
        "            \"å…§æ–‡\": content,\n",
        "            \"é€£çµ\": news_url\n",
        "        })\n",
        "\n",
        "    return news_list, current_log"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 156,
      "metadata": {
        "id": "7FNixHMARr13"
      },
      "outputs": [],
      "source": [
        "def åˆ†ææ–°èæ–‡æœ¬(texts, top_n=10):\n",
        "    tokenized_corpus = []\n",
        "    clean_pattern = re.compile(r'[0-9a-zA-Z\\t\\n\\r\\f\\v\\(\\)\\-\\+\\=\\[\\]\\{\\}\\<\\>@\\#\\$\\%\\^\\&\\*\\!\\~\\`\\?\\/\\\\]+')\n",
        "    for text in texts:\n",
        "        if pd.notna(text) and str(text).strip() not in ('ç„¡å…§æ–‡', ''):\n",
        "            cleaned_text = clean_pattern.sub('', str(text).strip())\n",
        "            if not cleaned_text: continue\n",
        "            korean_nouns = okt.nouns(cleaned_text)\n",
        "            stop_words_korean = ['ê¸°ì', 'ë‰´ìŠ¤', 'ì—°í•©', 'ì œê³µ', 'ë”°ë¥´', 'ìœ„í•´', 'í†µí•´', 'ì´ë²ˆ', 'ê·¸ê²ƒ', 'ì´ê²ƒ', 'ì €ê²ƒ', 'ì—ì„œ', 'ì—ê²Œ', 'ì˜¤ëŠ˜', 'ì‹œê°„', 'ì—¬ëŸ¬ë¶„']\n",
        "            filtered_words = [\n",
        "                word for word in korean_nouns\n",
        "                if len(word) > 1 and word.strip() and word not in stop_words_korean\n",
        "            ]\n",
        "            if not filtered_words: continue\n",
        "            tokenized_corpus.append(\" \".join(filtered_words))\n",
        "\n",
        "    if not tokenized_corpus:\n",
        "        return None, 0, \"\\nâŒ è™•ç†å¾Œçš„æœ‰æ•ˆæ–‡æœ¬ç‚ºç©ºï¼Œè«‹æª¢æŸ¥æ–‡ç« å…§å®¹ã€‚\"\n",
        "\n",
        "    # TF-IDF è¨ˆç®—\n",
        "    vectorizer = TfidfVectorizer(min_df=1, max_df=0.8)\n",
        "    tfidf_matrix = vectorizer.fit_transform(tokenized_corpus)\n",
        "    feature_names = vectorizer.get_feature_names_out()\n",
        "    sums = tfidf_matrix.sum(axis=0)\n",
        "\n",
        "    word_scores = []\n",
        "    for col, term in enumerate(feature_names):\n",
        "        word_scores.append((term, sums[0, col]))\n",
        "    word_scores.sort(key=lambda x: x[1], reverse=True)\n",
        "    top_words = word_scores[:top_n]\n",
        "\n",
        "    # Log è¼¸å‡ºæ ¼å¼åŒ–\n",
        "    log = \"\\n\" + \"=\" * 50 + \"\\n\"\n",
        "    log += f\"ğŸ”¥ **ã€æ–‡ç« å…§æ–‡éŸ“æ–‡ç†±è©åˆ†æçµæœã€‘ (å‰ {len(top_words)} å)** ğŸ”¥\\n\"\n",
        "    log += f\"ğŸ’¡ ç¸½å…±åˆ†æäº† **{len(tokenized_corpus)}** ç¯‡æ–‡ç« ã€‚\\n\"\n",
        "    log += \"-\" * 50 + \"\\n\"\n",
        "    for rank, (word, score) in enumerate(top_words, start=1):\n",
        "        log += f\"No. {rank}: **{word}** (ç¸½é—œéµæ€§åˆ†æ•¸: {score:.4f})\\n\"\n",
        "    log += \"=\" * 50 + \"\\n\"\n",
        "\n",
        "    # è¿”å›ç†±è©åˆ—è¡¨ã€æ–‡ç« è¨ˆæ•¸å’Œ Log\n",
        "    return top_words, len(tokenized_corpus), log"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 157,
      "metadata": {
        "id": "P2eTqbllR7Kr"
      },
      "outputs": [],
      "source": [
        "def ä½¿ç”¨_Gemini_API_ç†±è©åˆ†æ(top_words_list):\n",
        "    # å‡½æ•¸å…§å®¹åŒå‰\n",
        "    global GEMINI_CLIENT, GEMINI_MODEL, gemini_enabled\n",
        "\n",
        "    if not gemini_enabled:\n",
        "        return {\"æ´å¯Ÿæ‘˜è¦\": \"Gemini æœå‹™æœªå•Ÿç”¨\", \"çµè«–\": \"Gemini æœå‹™æœªå•Ÿç”¨\"}\n",
        "\n",
        "    if not top_words_list or len(top_words_list) == 0:\n",
        "        return {\"æ´å¯Ÿæ‘˜è¦\": \"ç†±è©åˆ—è¡¨ç‚ºç©ºï¼Œç„¡æ³•åˆ†æ\", \"çµè«–\": \"ç†±è©åˆ—è¡¨ç‚ºç©ºï¼Œç„¡æ³•åˆ†æ\"}\n",
        "\n",
        "    formatted_keywords = \"\\n\".join([f\"{rank+1}. {word} (åˆ†æ•¸: {score:.4f})\" for rank, (word, score) in enumerate(top_words_list)])\n",
        "\n",
        "    prompt = (\n",
        "        \"ä½ æ˜¯ä¸€ä½è³‡æ·±çš„æ•¸æ“šåˆ†æå¸«ï¼Œè«‹æ ¹æ“šä»¥ä¸‹æä¾›çš„ **20ç¯‡æ–‡ç« èˆ‡ç†±è©åˆ†æçµæœ (éŸ“æ–‡åè©èˆ‡å…¶é—œéµæ€§åˆ†æ•¸)**ï¼Œæä¾›å°ˆæ¥­çš„åˆ†æã€‚\\n\\n\"\n",
        "        \"è«‹æ ¹æ“šé€™äº›ç†±è©ï¼Œç¸½çµé€™20ç¯‡æ–‡ç« é›†çš„ä¸»è¦è¨è«–ä¸»é¡Œã€è¶¨å‹¢æˆ–å½±éŸ¿ï¼Œä¸¦ç”¨**ç¹é«”ä¸­æ–‡**æä¾›å…©é …åˆ†æçµæœï¼š\\n\\n\"\n",
        "        \"1. **äº”å¥æ´å¯Ÿæ‘˜è¦ (Insights)**ï¼šä»¥æ¢åˆ—å¼è¼¸å‡º 5 å€‹å®¢è§€ã€ç²¾ç…‰ã€ä¸”å…·æ´å¯Ÿæ€§çš„é‡é»ã€‚è«‹ä½¿ç”¨æ•¸å­—(å¦‚1ã€2ã€3...)ä½œç‚ºé–‹é ­ç¬¦è™Ÿã€‚\\n\\n\"\n",
        "        \"2. **ä¸€æ®µçµè«– (Conclusion)**ï¼šç”¨ä¸€æ®µè©±ç¸½çµé€™äº›ç†±è©æ‰€ä»£è¡¨çš„æ•´é«”è¶¨å‹¢å’Œæ½›åœ¨å½±éŸ¿ï¼Œå…§å®¹é•·åº¦è«‹åš´æ ¼æ§åˆ¶åœ¨ 110 åˆ° 130 å€‹ä¸­æ–‡å­—ã€‚\\n\\n\"\n",
        "        \"è«‹ç¢ºä¿ä½ çš„è¼¸å‡ºæ ¼å¼å¦‚ä¸‹ï¼š\\n\"\n",
        "        \"ã€æ´å¯Ÿæ‘˜è¦ã€‘\\n\"\n",
        "        \"[ç¬¬ 1 å¥æ´å¯Ÿ]\\n\"\n",
        "        \"[ç¬¬ 2 å¥æ´å¯Ÿ]\\n\"\n",
        "        \"[ç¬¬ 3 å¥æ´å¯Ÿ]\\n\"\n",
        "        \"[ç¬¬ 4 å¥æ´å¯Ÿ]\\n\"\n",
        "        \"[ç¬¬ 5 å¥æ´å¯Ÿ]\\n\"\n",
        "        \"ã€çµè«–ã€‘\\n\"\n",
        "        \"[ä¸€æ®µ 120 å­—å·¦å³çš„çµè«–å…§å®¹]\\n\\n\"\n",
        "        f\"ç†±è©åˆ†æçµæœ (å‰ {len(top_words_list)} å)ï¼š\\n{formatted_keywords}\"\n",
        "    )\n",
        "\n",
        "    try:\n",
        "        response = GEMINI_CLIENT.models.generate_content(\n",
        "            model=GEMINI_MODEL,\n",
        "            contents=prompt\n",
        "        )\n",
        "\n",
        "        full_text = response.text.strip()\n",
        "\n",
        "        insight_match = full_text.find(\"ã€æ´å¯Ÿæ‘˜è¦ã€‘\")\n",
        "        conclusion_match = full_text.find(\"ã€çµè«–ã€‘\")\n",
        "\n",
        "        if insight_match != -1 and conclusion_match != -1:\n",
        "            insights_raw = full_text[insight_match + len(\"ã€æ´å¯Ÿæ‘˜è¦ã€‘\"):conclusion_match].strip()\n",
        "            conclusion_raw = full_text[conclusion_match + len(\"ã€çµè«–\") + 1:].strip()\n",
        "\n",
        "            insights_formatted = insights_raw.replace('\\n', '\\n- ').strip()\n",
        "            if not insights_formatted.startswith('- '):\n",
        "                insights_formatted = '- ' + insights_formatted\n",
        "\n",
        "            return {\n",
        "                \"æ´å¯Ÿæ‘˜è¦\": insights_formatted,\n",
        "                \"çµè«–\": conclusion_raw\n",
        "            }\n",
        "        else:\n",
        "             return {\"æ´å¯Ÿæ‘˜è¦\": \"æ ¼å¼è§£æå¤±æ•—ï¼Œè«‹æª¢æŸ¥åŸå§‹è¼¸å‡º\", \"çµè«–\": full_text}\n",
        "\n",
        "    except Exception as e:\n",
        "        return {\"æ´å¯Ÿæ‘˜è¦\": f\"Gemini å‘¼å«å¤±æ•—: {str(e)}\", \"çµè«–\": f\"Gemini å‘¼å«å¤±æ•—: {str(e)}\"}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 158,
      "metadata": {
        "id": "MWZeKUziR-Jr"
      },
      "outputs": [],
      "source": [
        "def get_suffix_names(sheet_suffix):\n",
        "    # æ ¹æ“šå¾Œç¶´ç”¢ç”Ÿå·¥ä½œè¡¨åç¨±\n",
        "    suffix = f\" ({sheet_suffix})\" if sheet_suffix else \"\"\n",
        "    RESULT_SHEET_NAME = f\"çˆ¬èŸ²çµæœ{suffix}\"\n",
        "    TOP_WORDS_SHEET_NAME = f\"ç†±è©åˆ†æçµæœ{suffix}\"\n",
        "    AI_SUMMARY_SHEET_NAME = f\"AIç¸½çµ{suffix}\"\n",
        "    return RESULT_SHEET_NAME, TOP_WORDS_SHEET_NAME, AI_SUMMARY_SHEET_NAME\n",
        "\n",
        "def run_only_scraper(url, sheet_suffix):\n",
        "    full_log = \"--- èªè­‰ç‹€æ…‹ ---\\n\" + sheet_auth_log + \"\\n\" + gemini_auth_log + \"\\n\" + \"=\" * 20 + \"\\n\"\n",
        "\n",
        "    if not url:\n",
        "        return full_log + \"\\nâŒ éŒ¯èª¤ï¼šè«‹è¼¸å…¥æœ‰æ•ˆçš„ Naver æ–°èç¶²å€ã€‚\"\n",
        "\n",
        "    RESULT_SHEET_NAME, _, _ = get_suffix_names(sheet_suffix)\n",
        "\n",
        "    # 1. çˆ¬èŸ²\n",
        "    full_log += \"--- éšæ®µ 1/2: åŸ·è¡Œçˆ¬èŸ² ---\\n\"\n",
        "    news_data, crawl_log = çˆ¬å–_naver_æ–°è(url)\n",
        "    full_log += crawl_log\n",
        "\n",
        "    if not news_data:\n",
        "        full_log += \"\\nâŒ çˆ¬èŸ²å¤±æ•—ï¼šæœªæŠ“å–åˆ°ä»»ä½•æœ‰æ•ˆæ–°èæ•¸æ“šï¼Œæµç¨‹ä¸­æ­¢ã€‚\"\n",
        "        return full_log\n",
        "\n",
        "    # 2. å¯«å…¥åŸå§‹è³‡æ–™\n",
        "    full_log += \"\\n--- éšæ®µ 2/2: å¯«å…¥åŸå§‹æ•¸æ“š ---\\n\"\n",
        "    write_data_log = å„²å­˜_è‡³_Google_Sheet(news_data, SHEET_KEY, sheet_name=RESULT_SHEET_NAME)\n",
        "    full_log += write_data_log\n",
        "    full_log += \"\\n\\nâœ… **çˆ¬å–èˆ‡å¯«å…¥ Sheet æµç¨‹å®Œæˆï¼**\"\n",
        "\n",
        "    return full_log\n",
        "\n",
        "def run_tfidf_analysis(sheet_to_read, sheet_suffix):\n",
        "    full_log = \"--- èªè­‰ç‹€æ…‹ ---\\n\" + sheet_auth_log + \"\\n\" + \"=\" * 20 + \"\\n\"\n",
        "\n",
        "    if not sheet_to_read:\n",
        "        return full_log + \"\\nâŒ éŒ¯èª¤ï¼šè«‹è¼¸å…¥è¦è®€å–çš„å·¥ä½œè¡¨åç¨±ã€‚\"\n",
        "\n",
        "    _, TOP_WORDS_SHEET_NAME, _ = get_suffix_names(sheet_suffix)\n",
        "\n",
        "    # 1. è®€å–æ–‡ç« è³‡æ–™\n",
        "    df, read_log = å¾_Google_Sheet_è®€å–è³‡æ–™(SHEET_KEY, sheet_to_read)\n",
        "    full_log += read_log\n",
        "\n",
        "    if df.empty:\n",
        "        return full_log\n",
        "\n",
        "    # 2. æº–å‚™åˆ†ææ•¸æ“š\n",
        "    if 'å…§æ–‡' in df.columns:\n",
        "        data_for_analysis = df['å…§æ–‡'].tolist()\n",
        "    elif 'Content' in df.columns:\n",
        "        data_for_analysis = df['Content'].tolist()\n",
        "    else:\n",
        "        full_log += \"\\nâŒ éŒ¯èª¤ï¼šåœ¨å·¥ä½œè¡¨ **'{sheet_to_read}'** ä¸­æ‰¾ä¸åˆ°æ¬„ä½ **'å…§æ–‡'** æˆ– **'Content'**ï¼Œç„¡æ³•é€²è¡Œåˆ†æã€‚\"\n",
        "        return full_log\n",
        "\n",
        "    # 3. TF-IDF åˆ†æ\n",
        "    top_words_list, analyzed_count, tfidf_log = åˆ†ææ–°èæ–‡æœ¬(data_for_analysis, top_n=10)\n",
        "    full_log += tfidf_log\n",
        "\n",
        "    # 4. å¯«å…¥ç†±è©çµæœ\n",
        "    if top_words_list and analyzed_count > 0:\n",
        "        write_words_log = å„²å­˜_ç†±è©_è‡³_Google_Sheet(top_words_list, SHEET_KEY, sheet_name=TOP_WORDS_SHEET_NAME)\n",
        "        full_log += \"\\n\" + \"=\" * 20 + \"\\n\" + write_words_log\n",
        "\n",
        "    return full_log\n",
        "\n",
        "\n",
        "def run_ai_hotword_analysis(sheet_to_read, sheet_suffix):\n",
        "    full_log = \"--- èªè­‰ç‹€æ…‹ ---\\n\" + sheet_auth_log + \"\\n\" + gemini_auth_log + \"\\n\" + \"=\" * 20 + \"\\n\"\n",
        "\n",
        "    if not gemini_enabled:\n",
        "        return full_log + \"\\nâŒ éŒ¯èª¤ï¼šGemini API æœªå•Ÿç”¨ï¼Œç„¡æ³•é€²è¡Œ AI ç¸½çµåˆ†æã€‚\"\n",
        "\n",
        "    if not sheet_to_read:\n",
        "        return full_log + \"\\nâŒ éŒ¯èª¤ï¼šè«‹è¼¸å…¥è¦è®€å–æ–‡ç« å…§å®¹çš„å·¥ä½œè¡¨åç¨±ã€‚\"\n",
        "\n",
        "    _, _, AI_SUMMARY_SHEET_NAME = get_suffix_names(sheet_suffix)\n",
        "\n",
        "    # 1. è®€å–æ–‡ç« è³‡æ–™\n",
        "    df, read_log = å¾_Google_Sheet_è®€å–è³‡æ–™(SHEET_KEY, sheet_to_read)\n",
        "    full_log += read_log\n",
        "\n",
        "    if df.empty:\n",
        "        return full_log\n",
        "\n",
        "    # 2. æº–å‚™åˆ†ææ•¸æ“š\n",
        "    if 'å…§æ–‡' in df.columns:\n",
        "        data_for_analysis = df['å…§æ–‡'].tolist()\n",
        "    elif 'Content' in df.columns:\n",
        "        data_for_analysis = df['Content'].tolist()\n",
        "    else:\n",
        "        full_log += f\"\\nâŒ éŒ¯èª¤ï¼šåœ¨å·¥ä½œè¡¨ **'{sheet_to_read}'** ä¸­æ‰¾ä¸åˆ°æ¬„ä½ **'å…§æ–‡'** æˆ– **'Content'**ï¼Œç„¡æ³•é€²è¡Œåˆ†æã€‚\"\n",
        "        return full_log\n",
        "\n",
        "    # 3. TF-IDF åˆ†æï¼Œå–å¾—ç†±è©\n",
        "    top_words_list, analyzed_count, tfidf_log = åˆ†ææ–°èæ–‡æœ¬(data_for_analysis, top_n=10)\n",
        "    full_log += tfidf_log\n",
        "\n",
        "    if not top_words_list or analyzed_count == 0:\n",
        "        return full_log\n",
        "\n",
        "    # 4. å‘¼å« Gemini API\n",
        "    full_log += \"\\nğŸ§  **åŸ·è¡Œç†±è©ç¸½çµåˆ†æ (Gemini API)...**\"\n",
        "    time.sleep(0.5) # æ¨¡æ“¬æ€è€ƒæ™‚é–“\n",
        "    analysis_result = ä½¿ç”¨_Gemini_API_ç†±è©åˆ†æ(top_words_list)\n",
        "\n",
        "    # 5. å¯«å…¥ AI ç¸½çµçµæœ\n",
        "    write_summary_log = å„²å­˜_åˆ†æçµæœ_è‡³_Google_Sheet(analysis_result, SHEET_KEY, sheet_name=AI_SUMMARY_SHEET_NAME)\n",
        "    full_log += \"\\n\" + \"=\" * 20 + \"\\n\" + write_summary_log\n",
        "\n",
        "    # 6. æ•´ç† AI ç¸½çµè¼¸å‡º\n",
        "    ai_output = (\n",
        "        \"--- Gemini AI ç¸½çµåˆ†æçµæœ ---\\n\\n\"\n",
        "        f\"**Gemini æ´å¯Ÿæ‘˜è¦ï¼š**\\n{analysis_result['æ´å¯Ÿæ‘˜è¦']}\\n\\n\"\n",
        "        f\"**Gemini çµè«–ï¼š**\\n{analysis_result['çµè«–']}\"\n",
        "    )\n",
        "\n",
        "    return full_log, ai_output"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def run_all_in_one(url, sheet_suffix):\n",
        "    \"\"\"åˆ†é  4ï¼šçˆ¬èŸ²ã€TF-IDFã€AI ç¸½çµä¸€æ¬¡å®Œæˆ\"\"\"\n",
        "\n",
        "    full_log = \"--- èªè­‰ç‹€æ…‹ ---\\n\" + sheet_auth_log + \"\\n\" + gemini_auth_log + \"\\n\" + \"=\" * 20 + \"\\n\"\n",
        "    ai_output = \"åˆ†æå°šæœªåŸ·è¡Œã€‚\"\n",
        "\n",
        "    if not url:\n",
        "        return full_log + \"\\nâŒ éŒ¯èª¤ï¼šè«‹è¼¸å…¥æœ‰æ•ˆçš„ Naver æ–°èç¶²å€ã€‚\", ai_output\n",
        "\n",
        "    if not gemini_enabled:\n",
        "        return full_log + \"\\nâŒ éŒ¯èª¤ï¼šGemini API æœªå•Ÿç”¨ï¼Œç„¡æ³•é€²è¡Œ AI ç¸½çµåˆ†æã€‚\", ai_output\n",
        "\n",
        "    RESULT_SHEET_NAME, TOP_WORDS_SHEET_NAME, AI_SUMMARY_SHEET_NAME = get_suffix_names(sheet_suffix)\n",
        "\n",
        "    # 1. çˆ¬èŸ²\n",
        "    full_log += \"--- éšæ®µ 1/4: åŸ·è¡Œçˆ¬èŸ² ---\\n\"\n",
        "    news_data, crawl_log = çˆ¬å–_naver_æ–°è(url)\n",
        "    full_log += crawl_log\n",
        "\n",
        "    if not news_data:\n",
        "        return full_log, ai_output\n",
        "\n",
        "    # 2. å¯«å…¥åŸå§‹è³‡æ–™\n",
        "    full_log += \"\\n--- éšæ®µ 2/4: å¯«å…¥åŸå§‹æ•¸æ“š ---\\n\"\n",
        "    write_data_log = å„²å­˜_è‡³_Google_Sheet(news_data, SHEET_KEY, sheet_name=RESULT_SHEET_NAME)\n",
        "    full_log += write_data_log\n",
        "\n",
        "    # 3. TF-IDF åˆ†æ\n",
        "    full_log += \"\\n--- éšæ®µ 3/4: TF-IDF ç†±è©åˆ†æ ---\\n\"\n",
        "    data_for_analysis = [item['å…§æ–‡'] for item in news_data]\n",
        "    top_words_list, analyzed_count, tfidf_log = åˆ†ææ–°èæ–‡æœ¬(data_for_analysis, top_n=10)\n",
        "    full_log += tfidf_log\n",
        "\n",
        "    if not top_words_list or analyzed_count == 0:\n",
        "        return full_log + \"\\nâŒ ç”±æ–¼ç„¡æœ‰æ•ˆå…§æ–‡ï¼ŒAI ç¸½çµä¸­æ­¢ã€‚\", ai_output\n",
        "\n",
        "    # å¯«å…¥ç†±è©çµæœ\n",
        "    write_words_log = å„²å­˜_ç†±è©_è‡³_Google_Sheet(top_words_list, SHEET_KEY, sheet_name=TOP_WORDS_SHEET_NAME)\n",
        "    full_log += \"\\n\" + write_words_log\n",
        "\n",
        "\n",
        "    # 4. å‘¼å« Gemini API\n",
        "    full_log += \"\\n--- éšæ®µ 4/4: Gemini AI ç¸½çµ ---\\n\"\n",
        "    full_log += \"\\nğŸ§  **åŸ·è¡Œç†±è©ç¸½çµåˆ†æ (Gemini API)...**\"\n",
        "    time.sleep(0.5) # æ¨¡æ“¬æ€è€ƒæ™‚é–“\n",
        "    analysis_result = ä½¿ç”¨_Gemini_API_ç†±è©åˆ†æ(top_words_list)\n",
        "\n",
        "    # å¯«å…¥ AI ç¸½çµçµæœ\n",
        "    write_summary_log = å„²å­˜_åˆ†æçµæœ_è‡³_Google_Sheet(analysis_result, SHEET_KEY, sheet_name=AI_SUMMARY_SHEET_NAME)\n",
        "    full_log += \"\\n\" + write_summary_log\n",
        "\n",
        "    # æ•´ç† AI ç¸½çµè¼¸å‡º\n",
        "    ai_output = (\n",
        "        \"--- Gemini AI ç¸½çµåˆ†æçµæœ ---\\n\\n\"\n",
        "        f\"**Gemini æ´å¯Ÿæ‘˜è¦ï¼š**\\n{analysis_result['æ´å¯Ÿæ‘˜è¦']}\\n\\n\"\n",
        "        f\"**Gemini çµè«–ï¼š**\\n{analysis_result['çµè«–']}\"\n",
        "    )\n",
        "    full_log += \"\\n\\nâœ… **æ‰€æœ‰æ­¥é©ŸåŸ·è¡Œå®Œæˆï¼** çµæœå·²å¯«å…¥ Sheetã€‚\"\n",
        "\n",
        "    return full_log, ai_output"
      ],
      "metadata": {
        "id": "YmeWJWTs-FHl"
      },
      "execution_count": 159,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 166,
      "metadata": {
        "id": "H9OMdCRcSEHq"
      },
      "outputs": [],
      "source": [
        "with gr.Blocks(title=\"Naver æ–°èåˆ†æå·¥å…·\") as demo:\n",
        "    gr.Markdown(\"# Naver æ–°èåˆ†æå·¥å…·\")\n",
        "    gr.Markdown(\"### è«‹å‰å¾€ [Naver æ–°èä¸»é ](https://news.naver.com/) å°‹æ‰¾ç¶²å€ã€‚ä¹Ÿå¯é»é€²[Google Sheet](https://docs.google.com/spreadsheets/d/1P4V-D8o7bXHHHVMRQxG6voDjeyVUcDRB_PAahWajUUM/edit?usp=sharing)æŸ¥çœ‹è³‡æ–™\")\n",
        "\n",
        "    # å…±åŒè¼¸å…¥å€å¡Š\n",
        "    gr.Markdown(\"### å…±åŒè¼¸å…¥å€\")\n",
        "    with gr.Row():\n",
        "        sheet_suffix_input = gr.Textbox(\n",
        "            label=\"è‡ªè¨‚å·¥ä½œè¡¨åç¨±å¾Œç¶´\",\n",
        "            placeholder=\"ä¾‹å¦‚ï¼šMBCæ–°èã€‚å°‡ç”¨æ–¼å€åˆ†æ‰€æœ‰è¼¸å‡ºçš„å·¥ä½œè¡¨åç¨±ã€‚\",\n",
        "            scale=3\n",
        "        )\n",
        "        gr.Markdown(\"---\")\n",
        "\n",
        "    with gr.Accordion(\"å·¥ä½œè¡¨åˆªé™¤\", open=False):\n",
        "        delete_btn = gr.Button(\"ğŸš¨ åˆªé™¤æ‰€æœ‰ç›¸é—œå·¥ä½œè¡¨ (ä½¿ç”¨ä¸Šæ–¹å¾Œç¶´)\")\n",
        "        delete_output = gr.Textbox(\n",
        "            label=\"åˆªé™¤ä¸‰å¼µå·¥ä½œè¡¨\",\n",
        "            lines=5,\n",
        "            interactive=False\n",
        "        )\n",
        "\n",
        "        delete_btn.click(\n",
        "            fn=delete_sheets_by_suffix,\n",
        "            inputs=[sheet_suffix_input],\n",
        "            outputs=delete_output\n",
        "        )\n",
        "\n",
        "    # åŠŸèƒ½å€å¡Š\n",
        "    gr.Markdown(\"### åŠŸèƒ½å€\")\n",
        "\n",
        "    with gr.Tabs():\n",
        "\n",
        "        # 1. All in one (Option 1)\n",
        "        with gr.TabItem(\"çˆ¬èŸ²/ç†±è©åˆ†æ/AIç¸½çµ\"):\n",
        "            gr.Markdown(\"### ä¸€éµåŸ·è¡Œçˆ¬èŸ²ã€ç†±è©åˆ†æã€AI ç¸½çµ (å¿«é€Ÿç²å–çµæœ)\")\n",
        "\n",
        "            url_input_1 = gr.Textbox(\n",
        "                label=\"Naver æ–°èåª’é«”/ä¸»é¡Œç¶²å€\",\n",
        "                placeholder=\"ä¾‹å¦‚ï¼šhttps://media.naver.com/press/658?sid=102\"\n",
        "            )\n",
        "\n",
        "            all_in_one_btn = gr.Button(\"ğŸ”¥ é–‹å§‹ã€ä¸€éµå®Œæ•´åˆ†æã€‘ä¸¦å¯«å…¥ Sheet (è«‹è€å¿ƒç­‰å¾…)\")\n",
        "\n",
        "\n",
        "            with gr.Row():\n",
        "\n",
        "                all_in_one_log = gr.Textbox(\n",
        "                    label=\"åŸ·è¡Œçˆ¬èŸ²èˆ‡ç†±è©åˆ†æ\",\n",
        "                    lines=20,\n",
        "                    interactive=False\n",
        "                )\n",
        "\n",
        "                all_in_one_summary = gr.Textbox(\n",
        "                    label=\"Gemini AI ç¸½çµçµæœ\",\n",
        "                    lines=20,\n",
        "                    interactive=False\n",
        "                )\n",
        "\n",
        "            all_in_one_btn.click(\n",
        "                fn=run_all_in_one,\n",
        "                inputs=[url_input_1, sheet_suffix_input],\n",
        "                outputs=[all_in_one_log, all_in_one_summary]\n",
        "            )\n",
        "\n",
        "        # 2. çˆ¬å– Naver æ–°è (Option 2)\n",
        "        with gr.TabItem(\"çˆ¬å– Naver æ–°è\"):\n",
        "            gr.Markdown(\"### çˆ¬å– Naver æ–°è (åŸ·è¡Œçˆ¬èŸ²)\")\n",
        "\n",
        "            url_input_2 = gr.Textbox(\n",
        "                label=\"Naver æ–°èåª’é«”/ä¸»é¡Œç¶²å€\",\n",
        "                placeholder=\"ä¾‹å¦‚ï¼šhttps://media.naver.com/press/658?sid=102\"\n",
        "            )\n",
        "\n",
        "            scrape_btn = gr.Button(\"ğŸš€ é–‹å§‹çˆ¬å–ä¸¦å¯«å…¥ Sheet (çˆ¬èŸ²çµæœ)\")\n",
        "\n",
        "            scrape_output = gr.Textbox(\n",
        "                label=\"æ–‡ç« çˆ¬å–\",\n",
        "                lines=20,\n",
        "                interactive=False\n",
        "            )\n",
        "\n",
        "            scrape_btn.click(\n",
        "                fn=run_only_scraper,\n",
        "                inputs=[url_input_2, sheet_suffix_input],\n",
        "                outputs=scrape_output\n",
        "            )\n",
        "\n",
        "        # 3. è®€å– Google Sheet è³‡æ–™ (Option 3)\n",
        "        with gr.TabItem(\"è®€å– Sheet è³‡æ–™\"):\n",
        "            gr.Markdown(\"### è®€å– Google Sheet è³‡æ–™ (åƒ…åŸ·è¡Œ TF-IDF ç†±è©åˆ†æ)\")\n",
        "\n",
        "            read_tfidf_sheet_input = gr.Textbox(\n",
        "                label=\"è¦è®€å–çš„å·¥ä½œè¡¨åç¨±(é™çˆ¬èŸ²çµæœ)\",\n",
        "                placeholder=\"ä¾‹å¦‚ï¼šçˆ¬èŸ²çµæœ, çˆ¬èŸ²çµæœ (MBCæ–°è)\"\n",
        "            )\n",
        "\n",
        "            tfidf_btn = gr.Button(\"ğŸ“Š é–‹å§‹è®€å–ã€åˆ†æä¸¦å¯«å…¥ Sheet (ç†±è©åˆ†æçµæœ)\")\n",
        "\n",
        "            tfidf_output = gr.Textbox(\n",
        "                label=\"Sheetè®€å–ä¸¦åˆ†æç†±è©\",\n",
        "                lines=20,\n",
        "                interactive=False\n",
        "            )\n",
        "\n",
        "            tfidf_btn.click(\n",
        "                fn=run_tfidf_analysis,\n",
        "                inputs=[read_tfidf_sheet_input, sheet_suffix_input],\n",
        "                outputs=tfidf_output\n",
        "            )\n",
        "\n",
        "        # 4. AI åˆ†æ (Option 4)\n",
        "        with gr.TabItem(\"AI åˆ†æ (ç†±è©ç¸½çµ)\"):\n",
        "            gr.Markdown(\"### AI åˆ†æ (é‡å°çˆ¬å–æ–‡ç« çš„ç†±è©é€²è¡Œç¸½çµ)\")\n",
        "\n",
        "            ai_sheet_input = gr.Textbox(\n",
        "                label=\"è¦è®€å–çš„å·¥ä½œè¡¨åç¨±(é™çˆ¬èŸ²çµæœ)\",\n",
        "                placeholder=\"ä¾‹å¦‚ï¼šçˆ¬èŸ²çµæœ, çˆ¬èŸ²çµæœ (MBCæ–°è)\"\n",
        "            )\n",
        "\n",
        "            ai_btn = gr.Button(\"ğŸ§  é–‹å§‹ç†±è©ç¸½çµåˆ†æä¸¦å¯«å…¥ Sheet (AIç¸½çµ)\")\n",
        "\n",
        "            ai_output_log = gr.Textbox(\n",
        "                label=\"åˆ†æçˆ¬å–å…§å®¹èˆ‡ç†±è©\",\n",
        "                lines=10,\n",
        "                interactive=False\n",
        "            )\n",
        "\n",
        "            ai_output_summary = gr.Textbox(\n",
        "                label=\"Gemini AI ç¸½çµçµæœ\",\n",
        "                lines=10,\n",
        "                interactive=False\n",
        "            )\n",
        "\n",
        "            ai_btn.click(\n",
        "                fn=run_ai_hotword_analysis,\n",
        "                inputs=[ai_sheet_input, sheet_suffix_input],\n",
        "                outputs=[ai_output_log, ai_output_summary]\n",
        "            )\n",
        "\n",
        "\n",
        "        # 5. å·¥ä½œè¡¨åŒ¯å‡º (Option 5)\n",
        "        with gr.TabItem(\"å·¥ä½œè¡¨åŒ¯å‡º\"):\n",
        "            gr.Markdown(\"### åŒ¯å‡ºæŒ‡å®šå·¥ä½œè¡¨å…§å®¹ä¸¦æä¾›ä¸‹è¼‰\")\n",
        "\n",
        "            with gr.Accordion(\"ğŸ“¦ å–®ä¸€å·¥ä½œè¡¨åŒ¯å‡º\", open=False):\n",
        "              export_sheet_input = gr.Textbox(\n",
        "                label=\"è¦åŒ¯å‡ºè³‡æ–™çš„å–®ä¸€å·¥ä½œè¡¨åç¨±\",\n",
        "                placeholder=\"ä¾‹å¦‚ï¼šçˆ¬èŸ²çµæœ (MBCæ–°è), ç†±è©åˆ†æçµæœ (MBCæ–°è)ã€‚å·¥ä½œè¡¨åç¨±å¯åœ¨Google Sheetä¸­æŸ¥çœ‹\"\n",
        "              )\n",
        "\n",
        "              format_radio_single = gr.Radio(\n",
        "                [\"CSV\", \"JSON\"],\n",
        "                label=\"é¸æ“‡åŒ¯å‡ºæ ¼å¼\",\n",
        "                value=\"CSV\"\n",
        "              )\n",
        "\n",
        "              export_btn_single = gr.Button(\"â¬‡ï¸ è®€å–ä¸¦ç”¢ç”Ÿä¸‹è¼‰æª”æ¡ˆ (å–®ä¸€å·¥ä½œè¡¨)\")\n",
        "\n",
        "              with gr.Row():\n",
        "                export_file_output_single = gr.File(\n",
        "                    label=\"ä¸‹è¼‰æª”æ¡ˆ\",\n",
        "                    scale=1\n",
        "                )\n",
        "                export_log_output_single = gr.Textbox(\n",
        "                    label=\"åŒ¯å‡ºæª”æ¡ˆ\",\n",
        "                    lines=8,\n",
        "                    interactive=False,\n",
        "                    scale=2\n",
        "                )\n",
        "\n",
        "              export_btn_single.click(\n",
        "                fn=run_and_clear_single_export,\n",
        "                inputs=[export_sheet_input, format_radio_single],\n",
        "                outputs=[export_file_output_single, export_log_output_single, export_sheet_input]\n",
        "                )\n",
        "\n",
        "            gr.Markdown(\"---\")\n",
        "\n",
        "        # --- å€å¡Š 5.2: ä¸€éµåŒ¯å‡ºæ‰€æœ‰å·¥ä½œè¡¨ ---\n",
        "            with gr.Accordion(\"ğŸ—œï¸ ZIP å£“ç¸®åŒ…åŒ¯å‡º (ä½¿ç”¨å¾Œç¶´)\", open=False):\n",
        "              gr.Markdown(\"æ­¤åŠŸèƒ½å°‡ä½¿ç”¨ä¸Šæ–¹**å·¥ä½œè¡¨å‘½å**çš„ `å·¥ä½œè¡¨åç¨±å¾Œç¶´`ï¼Œå°‡ `çˆ¬èŸ²çµæœ`ã€`ç†±è©åˆ†æçµæœ`ã€`AIç¸½çµ` ä¸‰å€‹å·¥ä½œè¡¨å£“ç¸®æˆä¸€å€‹ ZIP æª”æ¡ˆã€‚\")\n",
        "\n",
        "              format_radio_all = gr.Radio(\n",
        "                [\"CSV\", \"JSON\"],\n",
        "                label=\"é¸æ“‡å£“ç¸®åŒ…å…§éƒ¨çš„æª”æ¡ˆæ ¼å¼\",\n",
        "                value=\"CSV\"\n",
        "              )\n",
        "\n",
        "              export_btn_all = gr.Button(\"ğŸ“¦ é–‹å§‹ã€ä¸€éµåŒ¯å‡ºã€‘æ‰€æœ‰ç›¸é—œå·¥ä½œè¡¨ (ZIP)\")\n",
        "\n",
        "              with gr.Row():\n",
        "                export_file_output_all = gr.File(\n",
        "                    label=\"ä¸‹è¼‰æª”æ¡ˆ (ZIPæª”)\",\n",
        "                    scale=1\n",
        "                )\n",
        "                export_log_output_all = gr.Textbox(\n",
        "                    label=\"åŒ¯å‡ºæª”æ¡ˆ\",\n",
        "                    lines=8,\n",
        "                    interactive=False,\n",
        "                    scale=2\n",
        "                )\n",
        "\n",
        "              export_btn_all.click(\n",
        "                fn=export_all_by_suffix,\n",
        "                inputs=[sheet_suffix_input, format_radio_all],\n",
        "                outputs=[export_file_output_all, export_log_output_all]\n",
        "                )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 167,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 680
        },
        "id": "osvWV8bOSG04",
        "outputId": "c4c56397-c7d3-435f-d777-3f593872b4bc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "It looks like you are running Gradio on a hosted Jupyter notebook, which requires `share=True`. Automatically setting `share=True` (you can turn this off by setting `share=False` in `launch()` explicitly).\n",
            "\n",
            "Colab notebook detected. This cell will run indefinitely so that you can see errors and logs. To turn off, set debug=False in launch().\n",
            "* Running on public URL: https://8e0f4cbb67025983f0.gradio.live\n",
            "\n",
            "This share link expires in 1 week. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://8e0f4cbb67025983f0.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Keyboard interruption in main thread... closing server.\n",
            "Killing tunnel 127.0.0.1:7860 <> https://8e0f4cbb67025983f0.gradio.live\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 167
        }
      ],
      "source": [
        "demo.launch(debug=True)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyObOcnkkg2BZExJRG3P/OZ0",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}