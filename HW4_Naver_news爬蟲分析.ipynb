{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/41371122h-lichi/lichi_thursday/blob/main/HW4_Naver_news%E7%88%AC%E8%9F%B2%E5%88%86%E6%9E%90.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UhVAbRkjRQr7"
      },
      "source": [
        "# Naver news分析 (作業四)\n",
        "\n",
        "**主題發想**\n",
        "\n",
        "因為最近在準備韓檢(已考完)，所以常常會需要閱讀新聞，就想說能不能把Naver的新聞拿來做分析，看看韓國人平常都接收到怎麼樣的資訊，也可以透過熱詞分析增加認識的詞彙量！\n",
        "\n",
        "**功能介紹**\n",
        "\n",
        "1.   爬蟲/熱詞分析/AI總結一步到位\n",
        "2.   純 · 爬取資料\n",
        "1.   純 · 熱詞分析\n",
        "2.   純 · AI總結\n",
        "1.   匯出CSV/JSON(可選擇單一/三張工作表匯出)\n",
        "2.   工作表命名/刪除\n",
        "\n",
        "**各項參考URL**\n",
        "\n",
        "· [Naver news](https://news.naver.com/)\n",
        "\n",
        "· [Sheet URL](https://docs.google.com/spreadsheets/d/1P4V-D8o7bXHHHVMRQxG6voDjeyVUcDRB_PAahWajUUM/edit?usp=sharing)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 142,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WVm0_myQRNGZ",
        "outputId": "2a259c80-18c4-4bef-c5ff-9faa2557402d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (2.32.4)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.12/dist-packages (4.13.5)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests) (2025.10.5)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.12/dist-packages (from beautifulsoup4) (2.8)\n",
            "Requirement already satisfied: typing-extensions>=4.0.0 in /usr/local/lib/python3.12/dist-packages (from beautifulsoup4) (4.15.0)\n"
          ]
        }
      ],
      "source": [
        "pip install requests beautifulsoup4"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 143,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wPE-GAxTRV_6",
        "outputId": "1d509e8f-d067-410c-9f95-7e91dd0cf1da"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: konlpy in /usr/local/lib/python3.12/dist-packages (0.6.0)\n",
            "Requirement already satisfied: JPype1>=0.7.0 in /usr/local/lib/python3.12/dist-packages (from konlpy) (1.6.0)\n",
            "Requirement already satisfied: lxml>=4.1.0 in /usr/local/lib/python3.12/dist-packages (from konlpy) (5.4.0)\n",
            "Requirement already satisfied: numpy>=1.6 in /usr/local/lib/python3.12/dist-packages (from konlpy) (2.0.2)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.12/dist-packages (from JPype1>=0.7.0->konlpy) (25.0)\n"
          ]
        }
      ],
      "source": [
        "pip install konlpy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 144,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9pvdjdtCRXs2",
        "outputId": "83a41033-e9e6-47bc-d650-5cdc2fb56e10"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: gspread_dataframe in /usr/local/lib/python3.12/dist-packages (4.0.0)\n",
            "Requirement already satisfied: gspread>=3.0.0 in /usr/local/lib/python3.12/dist-packages (from gspread_dataframe) (6.2.1)\n",
            "Requirement already satisfied: pandas>=0.24.0 in /usr/local/lib/python3.12/dist-packages (from gspread_dataframe) (2.2.2)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.12/dist-packages (from gspread_dataframe) (1.17.0)\n",
            "Requirement already satisfied: google-auth>=1.12.0 in /usr/local/lib/python3.12/dist-packages (from gspread>=3.0.0->gspread_dataframe) (2.38.0)\n",
            "Requirement already satisfied: google-auth-oauthlib>=0.4.1 in /usr/local/lib/python3.12/dist-packages (from gspread>=3.0.0->gspread_dataframe) (1.2.2)\n",
            "Requirement already satisfied: numpy>=1.26.0 in /usr/local/lib/python3.12/dist-packages (from pandas>=0.24.0->gspread_dataframe) (2.0.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas>=0.24.0->gspread_dataframe) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas>=0.24.0->gspread_dataframe) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas>=0.24.0->gspread_dataframe) (2025.2)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.12/dist-packages (from google-auth>=1.12.0->gspread>=3.0.0->gspread_dataframe) (5.5.2)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.12/dist-packages (from google-auth>=1.12.0->gspread>=3.0.0->gspread_dataframe) (0.4.2)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.12/dist-packages (from google-auth>=1.12.0->gspread>=3.0.0->gspread_dataframe) (4.9.1)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.12/dist-packages (from google-auth-oauthlib>=0.4.1->gspread>=3.0.0->gspread_dataframe) (2.0.0)\n",
            "Requirement already satisfied: pyasn1<0.7.0,>=0.6.1 in /usr/local/lib/python3.12/dist-packages (from pyasn1-modules>=0.2.1->google-auth>=1.12.0->gspread>=3.0.0->gspread_dataframe) (0.6.1)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.12/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib>=0.4.1->gspread>=3.0.0->gspread_dataframe) (3.3.1)\n",
            "Requirement already satisfied: requests>=2.0.0 in /usr/local/lib/python3.12/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib>=0.4.1->gspread>=3.0.0->gspread_dataframe) (2.32.4)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests>=2.0.0->requests-oauthlib>=0.7.0->google-auth-oauthlib>=0.4.1->gspread>=3.0.0->gspread_dataframe) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests>=2.0.0->requests-oauthlib>=0.7.0->google-auth-oauthlib>=0.4.1->gspread>=3.0.0->gspread_dataframe) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests>=2.0.0->requests-oauthlib>=0.7.0->google-auth-oauthlib>=0.4.1->gspread>=3.0.0->gspread_dataframe) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests>=2.0.0->requests-oauthlib>=0.7.0->google-auth-oauthlib>=0.4.1->gspread>=3.0.0->gspread_dataframe) (2025.10.5)\n"
          ]
        }
      ],
      "source": [
        "pip install gspread_dataframe"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 145,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SondYDHQRZRg",
        "outputId": "8bd4b930-f2d3-433f-b71f-6060dea8c666"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: google-genai in /usr/local/lib/python3.12/dist-packages (1.45.0)\n",
            "Requirement already satisfied: anyio<5.0.0,>=4.8.0 in /usr/local/lib/python3.12/dist-packages (from google-genai) (4.11.0)\n",
            "Requirement already satisfied: google-auth<3.0.0,>=2.14.1 in /usr/local/lib/python3.12/dist-packages (from google-genai) (2.38.0)\n",
            "Requirement already satisfied: httpx<1.0.0,>=0.28.1 in /usr/local/lib/python3.12/dist-packages (from google-genai) (0.28.1)\n",
            "Requirement already satisfied: pydantic<3.0.0,>=2.0.0 in /usr/local/lib/python3.12/dist-packages (from google-genai) (2.11.10)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.28.1 in /usr/local/lib/python3.12/dist-packages (from google-genai) (2.32.4)\n",
            "Requirement already satisfied: tenacity<9.2.0,>=8.2.3 in /usr/local/lib/python3.12/dist-packages (from google-genai) (8.5.0)\n",
            "Requirement already satisfied: websockets<15.1.0,>=13.0.0 in /usr/local/lib/python3.12/dist-packages (from google-genai) (15.0.1)\n",
            "Requirement already satisfied: typing-extensions<5.0.0,>=4.11.0 in /usr/local/lib/python3.12/dist-packages (from google-genai) (4.15.0)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.12/dist-packages (from anyio<5.0.0,>=4.8.0->google-genai) (3.11)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.12/dist-packages (from anyio<5.0.0,>=4.8.0->google-genai) (1.3.1)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.12/dist-packages (from google-auth<3.0.0,>=2.14.1->google-genai) (5.5.2)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.12/dist-packages (from google-auth<3.0.0,>=2.14.1->google-genai) (0.4.2)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.12/dist-packages (from google-auth<3.0.0,>=2.14.1->google-genai) (4.9.1)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.12/dist-packages (from httpx<1.0.0,>=0.28.1->google-genai) (2025.10.5)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx<1.0.0,>=0.28.1->google-genai) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.12/dist-packages (from httpcore==1.*->httpx<1.0.0,>=0.28.1->google-genai) (0.16.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<3.0.0,>=2.0.0->google-genai) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.12/dist-packages (from pydantic<3.0.0,>=2.0.0->google-genai) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<3.0.0,>=2.0.0->google-genai) (0.4.2)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.28.1->google-genai) (3.4.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.28.1->google-genai) (2.5.0)\n",
            "Requirement already satisfied: pyasn1<0.7.0,>=0.6.1 in /usr/local/lib/python3.12/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3.0.0,>=2.14.1->google-genai) (0.6.1)\n"
          ]
        }
      ],
      "source": [
        "pip install google-genai"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 146,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z0Aur1oxRavI",
        "outputId": "44e4dd01-8853-4ffe-9c46-bf62cd47ad10"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: gradio in /usr/local/lib/python3.12/dist-packages (5.49.1)\n",
            "Requirement already satisfied: aiofiles<25.0,>=22.0 in /usr/local/lib/python3.12/dist-packages (from gradio) (24.1.0)\n",
            "Requirement already satisfied: anyio<5.0,>=3.0 in /usr/local/lib/python3.12/dist-packages (from gradio) (4.11.0)\n",
            "Requirement already satisfied: brotli>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from gradio) (1.1.0)\n",
            "Requirement already satisfied: fastapi<1.0,>=0.115.2 in /usr/local/lib/python3.12/dist-packages (from gradio) (0.119.0)\n",
            "Requirement already satisfied: ffmpy in /usr/local/lib/python3.12/dist-packages (from gradio) (0.6.3)\n",
            "Requirement already satisfied: gradio-client==1.13.3 in /usr/local/lib/python3.12/dist-packages (from gradio) (1.13.3)\n",
            "Requirement already satisfied: groovy~=0.1 in /usr/local/lib/python3.12/dist-packages (from gradio) (0.1.2)\n",
            "Requirement already satisfied: httpx<1.0,>=0.24.1 in /usr/local/lib/python3.12/dist-packages (from gradio) (0.28.1)\n",
            "Requirement already satisfied: huggingface-hub<2.0,>=0.33.5 in /usr/local/lib/python3.12/dist-packages (from gradio) (0.35.3)\n",
            "Requirement already satisfied: jinja2<4.0 in /usr/local/lib/python3.12/dist-packages (from gradio) (3.1.6)\n",
            "Requirement already satisfied: markupsafe<4.0,>=2.0 in /usr/local/lib/python3.12/dist-packages (from gradio) (3.0.3)\n",
            "Requirement already satisfied: numpy<3.0,>=1.0 in /usr/local/lib/python3.12/dist-packages (from gradio) (2.0.2)\n",
            "Requirement already satisfied: orjson~=3.0 in /usr/local/lib/python3.12/dist-packages (from gradio) (3.11.3)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.12/dist-packages (from gradio) (25.0)\n",
            "Requirement already satisfied: pandas<3.0,>=1.0 in /usr/local/lib/python3.12/dist-packages (from gradio) (2.2.2)\n",
            "Requirement already satisfied: pillow<12.0,>=8.0 in /usr/local/lib/python3.12/dist-packages (from gradio) (11.3.0)\n",
            "Requirement already satisfied: pydantic<2.12,>=2.0 in /usr/local/lib/python3.12/dist-packages (from gradio) (2.11.10)\n",
            "Requirement already satisfied: pydub in /usr/local/lib/python3.12/dist-packages (from gradio) (0.25.1)\n",
            "Requirement already satisfied: python-multipart>=0.0.18 in /usr/local/lib/python3.12/dist-packages (from gradio) (0.0.20)\n",
            "Requirement already satisfied: pyyaml<7.0,>=5.0 in /usr/local/lib/python3.12/dist-packages (from gradio) (6.0.3)\n",
            "Requirement already satisfied: ruff>=0.9.3 in /usr/local/lib/python3.12/dist-packages (from gradio) (0.14.1)\n",
            "Requirement already satisfied: safehttpx<0.2.0,>=0.1.6 in /usr/local/lib/python3.12/dist-packages (from gradio) (0.1.6)\n",
            "Requirement already satisfied: semantic-version~=2.0 in /usr/local/lib/python3.12/dist-packages (from gradio) (2.10.0)\n",
            "Requirement already satisfied: starlette<1.0,>=0.40.0 in /usr/local/lib/python3.12/dist-packages (from gradio) (0.48.0)\n",
            "Requirement already satisfied: tomlkit<0.14.0,>=0.12.0 in /usr/local/lib/python3.12/dist-packages (from gradio) (0.13.3)\n",
            "Requirement already satisfied: typer<1.0,>=0.12 in /usr/local/lib/python3.12/dist-packages (from gradio) (0.19.2)\n",
            "Requirement already satisfied: typing-extensions~=4.0 in /usr/local/lib/python3.12/dist-packages (from gradio) (4.15.0)\n",
            "Requirement already satisfied: uvicorn>=0.14.0 in /usr/local/lib/python3.12/dist-packages (from gradio) (0.37.0)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.12/dist-packages (from gradio-client==1.13.3->gradio) (2025.3.0)\n",
            "Requirement already satisfied: websockets<16.0,>=13.0 in /usr/local/lib/python3.12/dist-packages (from gradio-client==1.13.3->gradio) (15.0.1)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.12/dist-packages (from anyio<5.0,>=3.0->gradio) (3.11)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.12/dist-packages (from anyio<5.0,>=3.0->gradio) (1.3.1)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.12/dist-packages (from httpx<1.0,>=0.24.1->gradio) (2025.10.5)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx<1.0,>=0.24.1->gradio) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.12/dist-packages (from httpcore==1.*->httpx<1.0,>=0.24.1->gradio) (0.16.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<2.0,>=0.33.5->gradio) (3.20.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<2.0,>=0.33.5->gradio) (2.32.4)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<2.0,>=0.33.5->gradio) (4.67.1)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<2.0,>=0.33.5->gradio) (1.1.10)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas<3.0,>=1.0->gradio) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas<3.0,>=1.0->gradio) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas<3.0,>=1.0->gradio) (2025.2)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<2.12,>=2.0->gradio) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.12/dist-packages (from pydantic<2.12,>=2.0->gradio) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<2.12,>=2.0->gradio) (0.4.2)\n",
            "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.12/dist-packages (from typer<1.0,>=0.12->gradio) (8.3.0)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.12/dist-packages (from typer<1.0,>=0.12->gradio) (1.5.4)\n",
            "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.12/dist-packages (from typer<1.0,>=0.12->gradio) (13.9.4)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->pandas<3.0,>=1.0->gradio) (1.17.0)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.12/dist-packages (from rich>=10.11.0->typer<1.0,>=0.12->gradio) (4.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.12/dist-packages (from rich>=10.11.0->typer<1.0,>=0.12->gradio) (2.19.2)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface-hub<2.0,>=0.33.5->gradio) (3.4.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface-hub<2.0,>=0.33.5->gradio) (2.5.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.12/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0,>=0.12->gradio) (0.1.2)\n"
          ]
        }
      ],
      "source": [
        "pip install gradio"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 147,
      "metadata": {
        "id": "nflfR4R8RcOL"
      },
      "outputs": [],
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import pandas as pd\n",
        "import re\n",
        "from konlpy.tag import Okt\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "import time\n",
        "import tempfile\n",
        "import os\n",
        "import zipfile\n",
        "import shutil\n",
        "\n",
        "from google.colab import auth\n",
        "from google.auth import default\n",
        "import gspread\n",
        "from gspread_dataframe import set_with_dataframe\n",
        "import gspread.exceptions\n",
        "\n",
        "from google import genai\n",
        "from google.genai import types\n",
        "\n",
        "import gradio as gr\n",
        "\n",
        "okt = Okt()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 148,
      "metadata": {
        "id": "yzvWEaR7RfER"
      },
      "outputs": [],
      "source": [
        "try:\n",
        "    auth.authenticate_user()\n",
        "    creds, _ = default()\n",
        "    gc = gspread.authorize(creds)\n",
        "    sheets_enabled = True\n",
        "    sheet_auth_log = \"✅ Google Sheets 認證成功。\"\n",
        "except Exception as e:\n",
        "    sheet_auth_log = f\"❌ Google Sheets 認證失敗: {e}\"\n",
        "    sheets_enabled = False\n",
        "    gc = None\n",
        "\n",
        "# 2. Gemini API 認證 (使用您檔案中提供的金鑰和模型)\n",
        "GEMINI_API_KEY = \"AIzaSyBufFzIl5hJR-mPetOWfBxR7NnN8lPZHLM\"\n",
        "GEMINI_MODEL_NAME = \"gemini-2.5-flash\" # 建議使用 flash 模型以提高速度\n",
        "\n",
        "try:\n",
        "    # 設置 API 金鑰\n",
        "    GEMINI_CLIENT = genai.Client(api_key=GEMINI_API_KEY)\n",
        "    GEMINI_MODEL = GEMINI_MODEL_NAME\n",
        "\n",
        "    gemini_auth_log = f\"✅ Gemini API 客戶端初始化成功，模型設定為 {GEMINI_MODEL}。\"\n",
        "    gemini_enabled = True\n",
        "except Exception as e:\n",
        "    gemini_auth_log = f\"❌ Gemini API 初始化失敗 (請確認金鑰是否正確): {e}\"\n",
        "    GEMINI_CLIENT = None\n",
        "    gemini_enabled = False"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 149,
      "metadata": {
        "id": "owqT_EG4Rh08"
      },
      "outputs": [],
      "source": [
        "HEADERS = {\n",
        "    \"User-Agent\": (\n",
        "        \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) \"\n",
        "        \"AppleWebKit/537.36 (KHTML, like Gecko) \"\n",
        "        \"Chrome/114.0.0.0 Safari/537.36\"\n",
        "    )\n",
        "}\n",
        "\n",
        "SHEET_KEY = \"1P4V-D8o7bXHHHVMRQxG6voDjeyVUcDRB_PAahWajUUM\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 150,
      "metadata": {
        "id": "ncsfRzG9Rk2Z"
      },
      "outputs": [],
      "source": [
        "def 儲存_至_Google_Sheet(data_list, sheet_key, sheet_name=\"爬蟲結果\"):\n",
        "    global gc, sheets_enabled\n",
        "\n",
        "    if not sheets_enabled:\n",
        "        return \"❌ Google Sheets 認證未成功或權限不足，無法寫入資料。\"\n",
        "\n",
        "    if not data_list:\n",
        "        return \"❌ 資料清單為空，未寫入 Google Sheet。\"\n",
        "\n",
        "    try:\n",
        "        sheet = gc.open_by_key(sheet_key)\n",
        "        df = pd.DataFrame(data_list)\n",
        "        try:\n",
        "            worksheet = sheet.worksheet(sheet_name)\n",
        "        except gspread.WorksheetNotFound:\n",
        "            worksheet = sheet.add_worksheet(title=sheet_name, rows=max(len(df) + 1, 100), cols=max(len(df.columns), 20))\n",
        "\n",
        "        set_with_dataframe(worksheet, df)\n",
        "\n",
        "        return f\"✅ 成功寫入 **{len(data_list)}** 筆資料至 Google Sheet！\\n工作表名稱: **{sheet_name}**\"\n",
        "\n",
        "    except Exception as e:\n",
        "        return f\"❌ 寫入 Google Sheet 發生錯誤 (請檢查權限): {e}\"\n",
        "\n",
        "def 儲存_熱詞_至_Google_Sheet(word_scores, sheet_key, sheet_name=\"熱詞分析結果\"):\n",
        "    global gc, sheets_enabled\n",
        "\n",
        "    if not sheets_enabled or not word_scores:\n",
        "        return \"❌ 無法寫入熱詞資料。\"\n",
        "\n",
        "    try:\n",
        "        sheet = gc.open_by_key(sheet_key)\n",
        "        df = pd.DataFrame(word_scores, columns=['熱詞', '總關鍵性分數'])\n",
        "        df['排名'] = df.index + 1\n",
        "        df = df[['排名', '熱詞', '總關鍵性分數']]\n",
        "        try:\n",
        "            worksheet = sheet.worksheet(sheet_name)\n",
        "        except gspread.WorksheetNotFound:\n",
        "            worksheet = sheet.add_worksheet(title=sheet_name, rows=max(len(df) + 1, 100), cols=max(len(df.columns), 20))\n",
        "\n",
        "        set_with_dataframe(worksheet, df)\n",
        "\n",
        "        return f\"✅ 成功寫入 **{len(word_scores)}** 筆熱詞資料至 Google Sheet！\\n工作表名稱: **{sheet_name}**\"\n",
        "\n",
        "    except Exception as e:\n",
        "        return f\"❌ 寫入熱詞至 Google Sheet 發生錯誤 (請檢查權限): {e}\"\n",
        "\n",
        "def 儲存_分析結果_至_Google_Sheet(analysis_dict, sheet_key, sheet_name):\n",
        "    global gc, sheets_enabled\n",
        "\n",
        "    if not sheets_enabled or not analysis_dict:\n",
        "        return \"❌ 無法寫入分析結果。\"\n",
        "\n",
        "    try:\n",
        "        sheet = gc.open_by_key(sheet_key)\n",
        "        data = {\n",
        "            '分析主題': ['熱詞總體趨勢'],\n",
        "            'Gemini_洞察摘要': [analysis_dict.get('洞察摘要', '')],\n",
        "            'Gemini_結論': [analysis_dict.get('結論', '')]\n",
        "        }\n",
        "        df = pd.DataFrame(data)\n",
        "\n",
        "        try:\n",
        "            worksheet = sheet.worksheet(sheet_name)\n",
        "        except gspread.WorksheetNotFound:\n",
        "            worksheet = sheet.add_worksheet(title=sheet_name, rows=max(len(df) + 1, 10), cols=max(len(df.columns), 10))\n",
        "\n",
        "        set_with_dataframe(worksheet, df)\n",
        "\n",
        "        return f\"✅ 成功寫入 Gemini 分析結果至 Google Sheet！\\n工作表名稱: **{sheet_name}**\"\n",
        "\n",
        "    except Exception as e:\n",
        "        return f\"❌ 寫入分析結果至 Google Sheet 發生錯誤 (請檢查權限)：{e}\"\n",
        "\n",
        "\n",
        "def 從_Google_Sheet_讀取資料(sheet_key, sheet_name):\n",
        "    global gc, sheets_enabled\n",
        "    if not sheets_enabled:\n",
        "        return pd.DataFrame(), \"❌ Google Sheets 認證未成功或權限不足，無法讀取資料。\"\n",
        "    try:\n",
        "        sheet = gc.open_by_key(sheet_key)\n",
        "        worksheet = sheet.worksheet(sheet_name)\n",
        "        data = worksheet.get_all_records()\n",
        "        df = pd.DataFrame(data)\n",
        "        log = f\"✅ 成功讀取 **{len(df)}** 筆資料。\\n\"\n",
        "        return df, log\n",
        "    except gspread.exceptions.WorksheetNotFound:\n",
        "        return pd.DataFrame(), f\"❌ 讀取 Google Sheet 發生錯誤：找不到工作表 **'{sheet_name}'**，請確認名稱是否正確。\"\n",
        "    except Exception as e:\n",
        "        return pd.DataFrame(), f\"❌ 讀取 Google Sheet 發生錯誤：{e}\""
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def delete_sheets_by_suffix(sheet_suffix):\n",
        "    \"\"\"根據後綴，刪除相關的三個工作表\"\"\"\n",
        "    global gc, sheets_enabled\n",
        "    full_log = \"--- 認證狀態 ---\\n\" + sheet_auth_log + \"\\n\" + \"=\" * 20 + \"\\n\"\n",
        "\n",
        "    if not sheets_enabled:\n",
        "        return full_log + \"\\n❌ Google Sheets 認證未成功，無法刪除工作表。\"\n",
        "\n",
        "    if not sheet_suffix:\n",
        "        return full_log + \"\\n❌ 錯誤：請輸入有效的**工作表後綴**來指定要刪除的集合。\"\n",
        "\n",
        "    RESULT_SHEET_NAME, TOP_WORDS_SHEET_NAME, AI_SUMMARY_SHEET_NAME = get_suffix_names(sheet_suffix)\n",
        "\n",
        "    target_sheet_names = [RESULT_SHEET_NAME, TOP_WORDS_SHEET_NAME, AI_SUMMARY_SHEET_NAME]\n",
        "\n",
        "    delete_count = 0\n",
        "\n",
        "    try:\n",
        "        sheet = gc.open_by_key(SHEET_KEY)\n",
        "        full_log += f\"💡 正在嘗試刪除與後綴 **'{sheet_suffix}'** 相關的工作表：\\n\"\n",
        "\n",
        "        for sheet_name in target_sheet_names:\n",
        "            try:\n",
        "                worksheet = sheet.worksheet(sheet_name)\n",
        "                sheet.del_worksheet(worksheet)\n",
        "                full_log += f\"✅ 成功刪除工作表: **{sheet_name}**\\n\"\n",
        "                delete_count += 1\n",
        "            except gspread.WorksheetNotFound:\n",
        "                full_log += f\"⚠ 警告：找不到工作表: **{sheet_name}** (跳過)\\n\"\n",
        "\n",
        "        if delete_count == 0:\n",
        "            full_log += \"\\n❌ 刪除完成，但未找到任何相關工作表可供刪除。\"\n",
        "        else:\n",
        "            full_log += f\"\\n✅ **成功刪除 {delete_count} 個工作表！**\"\n",
        "\n",
        "    except Exception as e:\n",
        "        full_log += f\"\\n❌ 刪除過程中發生錯誤: {e}\"\n",
        "\n",
        "    return full_log"
      ],
      "metadata": {
        "id": "pGzEBbxDDjBg"
      },
      "execution_count": 151,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def export_sheet_to_csv_json(sheet_name, format_choice):\n",
        "    \"\"\"讀取指定工作表並匯出為 CSV 或 JSON 檔案\"\"\"\n",
        "    global gc, sheets_enabled\n",
        "\n",
        "    if not sheets_enabled:\n",
        "        return None, \"❌ Google Sheets 認證未成功，無法讀取工作表。\"\n",
        "\n",
        "    if not sheet_name:\n",
        "        return None, \"❌ 錯誤：請輸入要匯出的工作表名稱。\"\n",
        "\n",
        "    df, read_log = 從_Google_Sheet_讀取資料(SHEET_KEY, sheet_name)\n",
        "\n",
        "    if df.empty:\n",
        "        return None, read_log + \"\\n❌ 資料為空或讀取失敗，無法匯出。\"\n",
        "\n",
        "    # 創建一個臨時文件來儲存匯出的內容\n",
        "    with tempfile.NamedTemporaryFile(mode='w', delete=False, encoding='utf-8') as tmp_file:\n",
        "        file_path = tmp_file.name\n",
        "\n",
        "        if format_choice == 'CSV':\n",
        "            # 將 DataFrame 寫入 CSV，不包含索引，確保編碼\n",
        "            df.to_csv(file_path, index=False, encoding='utf-8')\n",
        "            output_log = f\"✅ 成功將 **{sheet_name}** ({len(df)} 筆資料) 匯出為 CSV 檔案。\"\n",
        "        elif format_choice == 'JSON':\n",
        "            # 將 DataFrame 寫入 JSON (orient='records' 格式為 [{...}, {...}])\n",
        "            df.to_json(file_path, orient='records', force_ascii=False, indent=4)\n",
        "            output_log = f\"✅ 成功將 **{sheet_name}** ({len(df)} 筆資料) 匯出為 JSON 檔案。\"\n",
        "        else:\n",
        "             return None, \"❌ 錯誤：無效的匯出格式選擇。\"\n",
        "\n",
        "    # Gradio 會將此文件路徑作為下載連結提供\n",
        "    return file_path, output_log"
      ],
      "metadata": {
        "id": "LAb-HFv8Djpc"
      },
      "execution_count": 152,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def run_and_clear_single_export(sheet_name, format_choice):\n",
        "    \"\"\"\n",
        "    包裝單一工作表匯出函數，並在完成後清空輸入框。\n",
        "    \"\"\"\n",
        "    file_path, output_log = export_sheet_to_csv_json(sheet_name, format_choice)\n",
        "\n",
        "    # 必須依序回傳給 outputs 列表：\n",
        "    # (1) 下載檔案, (2) Log 訊息, (3) 清空輸入框的值\n",
        "    return file_path, output_log, \"\""
      ],
      "metadata": {
        "id": "U1RQs_fvJBWs"
      },
      "execution_count": 153,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def export_all_by_suffix(sheet_suffix, format_choice):\n",
        "    \"\"\"根據後綴，將三個相關工作表讀取並壓縮成單一 ZIP 檔案供下載。\"\"\"\n",
        "    global gc, sheets_enabled\n",
        "\n",
        "    if not sheets_enabled:\n",
        "        return None, \"❌ Google Sheets 認證未成功，無法讀取工作表。\"\n",
        "\n",
        "    if not sheet_suffix:\n",
        "        return None, \"❌ 錯誤：請輸入要匯出的**工作表後綴**。\"\n",
        "\n",
        "    # 取得三個工作表名稱\n",
        "    RESULT_SHEET_NAME, TOP_WORDS_SHEET_NAME, AI_SUMMARY_SHEET_NAME = get_suffix_names(sheet_suffix)\n",
        "    target_sheet_names = {\n",
        "        '原始數據': RESULT_SHEET_NAME,\n",
        "        '熱詞分析': TOP_WORDS_SHEET_NAME,\n",
        "        'AI總結': AI_SUMMARY_SHEET_NAME\n",
        "    }\n",
        "\n",
        "    # 創建一個臨時目錄來存放 CSV/JSON 檔案\n",
        "    temp_dir = tempfile.mkdtemp()\n",
        "\n",
        "    exported_files_count = 0\n",
        "    log_messages = []\n",
        "\n",
        "    try:\n",
        "        sheet = gc.open_by_key(SHEET_KEY)\n",
        "\n",
        "        for file_type, sheet_name in target_sheet_names.items():\n",
        "            try:\n",
        "                # 1. 讀取資料\n",
        "                worksheet = sheet.worksheet(sheet_name)\n",
        "                data = worksheet.get_all_records()\n",
        "                df = pd.DataFrame(data)\n",
        "\n",
        "                if df.empty:\n",
        "                    log_messages.append(f\"⚠ 警告：工作表 **{sheet_name}** ({file_type}) 為空，跳過匯出。\")\n",
        "                    continue\n",
        "\n",
        "                # 2. 寫入暫存檔案\n",
        "                ext = 'csv' if format_choice == 'CSV' else 'json'\n",
        "                base_file_name = f\"{sheet_name}.{ext}\"\n",
        "                file_path = os.path.join(temp_dir, base_file_name)\n",
        "\n",
        "                if format_choice == 'CSV':\n",
        "                    df.to_csv(file_path, index=False, encoding='utf-8')\n",
        "                elif format_choice == 'JSON':\n",
        "                    df.to_json(file_path, orient='records', force_ascii=False, indent=4)\n",
        "\n",
        "                exported_files_count += 1\n",
        "                log_messages.append(f\"✅ 成功處理工作表 **{sheet_name}** ({len(df)} 筆資料)。\")\n",
        "\n",
        "            except gspread.exceptions.WorksheetNotFound:\n",
        "                log_messages.append(f\"⚠ 警告：找不到工作表 **{sheet_name}**，跳過匯出。\")\n",
        "            except Exception as e:\n",
        "                log_messages.append(f\"❌ 匯出工作表 **{sheet_name}** 時發生錯誤: {e}\")\n",
        "\n",
        "        # 3. 創建 ZIP 壓縮檔案\n",
        "        if exported_files_count == 0:\n",
        "            return None, \"\\n\".join(log_messages) + \"\\n❌ 沒有找到任何工作表可供匯出，操作失敗。\"\n",
        "\n",
        "        zip_filename = f\"Naver_Analysis_Export_{sheet_suffix}.zip\"\n",
        "        # 使用 tempfile.gettempdir() 確保 ZIP 檔案在可存取的位置\n",
        "        zip_path = os.path.join(tempfile.gettempdir(), zip_filename)\n",
        "\n",
        "        with zipfile.ZipFile(zip_path, 'w', zipfile.ZIP_DEFLATED) as zipf:\n",
        "            # 遍歷臨時目錄中的所有檔案並加入 ZIP\n",
        "            for file_name in os.listdir(temp_dir):\n",
        "                 file_path = os.path.join(temp_dir, file_name)\n",
        "                 zipf.write(file_path, file_name) # 第二個參數是 ZIP 內部的檔案名稱\n",
        "\n",
        "        final_log = \"\\n\".join(log_messages)\n",
        "        final_log += f\"\\n\\n✅ **操作完成！** 成功將 {exported_files_count} 個檔案打包成 ZIP 供下載。\"\n",
        "\n",
        "        return zip_path, final_log\n",
        "\n",
        "    except Exception as e:\n",
        "        return None, f\"❌ 匯出過程中發生嚴重錯誤: {e}\"\n",
        "\n",
        "    finally:\n",
        "        # 無論成功或失敗，都要清理臨時目錄\n",
        "        shutil.rmtree(temp_dir)"
      ],
      "metadata": {
        "id": "-9ijepX-Fbee"
      },
      "execution_count": 154,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 155,
      "metadata": {
        "id": "qRCDCtHtRo2b"
      },
      "outputs": [],
      "source": [
        "def 爬取_文章內文(article_url):\n",
        "    # 函數內容同前\n",
        "    if not article_url:\n",
        "        return \"無連結\", \"無連結\", \"無內文\"\n",
        "    try:\n",
        "        response = requests.get(article_url, headers=HEADERS, timeout=10)\n",
        "        response.raise_for_status()\n",
        "        soup = BeautifulSoup(response.text, \"html.parser\")\n",
        "        content_tag = soup.select_one(\"#dic_area, .news_content, #articeBody, .article_body, .story_view .ct_area\")\n",
        "        if content_tag:\n",
        "          for tag in content_tag.find_all(['script', 'a', 'img', 'br', 'em', 'strong', 'iframe', 'figcaption']):\n",
        "              tag.decompose()\n",
        "          content = content_tag.get_text(strip=True)\n",
        "        else:\n",
        "          content = \"無內文\"\n",
        "        author_tag = soup.select_one(\"em.media_end_head_journalist_name, .media_end_head_journalist_name, .byline .journalist_name\")\n",
        "        author = author_tag.get_text(strip=True) if author_tag else \"無作者\"\n",
        "        date_tag = soup.select_one(\"span.media_end_head_info_datestamp_time._ARTICLE_DATE_TIME, .info span.date\")\n",
        "        date = date_tag['data-date-time'] if date_tag and 'data-date-time' in date_tag.attrs else (\n",
        "            date_tag.get_text(strip=True) if date_tag else \"無日期\"\n",
        "        )\n",
        "        date = date.replace('입력 ', '').strip()\n",
        "        return date, author, content\n",
        "    except Exception as e:\n",
        "        return f\"內文頁爬取失敗: {type(e).__name__}\", \"內文頁爬取失敗\", \"無內文\"\n",
        "\n",
        "def 爬取_naver_新聞(url):\n",
        "    # 函數內容同前\n",
        "    try:\n",
        "        response = requests.get(url, headers=HEADERS)\n",
        "        response.raise_for_status()\n",
        "    except Exception as e:\n",
        "        return [], f\"❌ 主列表爬取失敗：請檢查網址或連線問題。\\n錯誤：{e}\"\n",
        "\n",
        "    soup = BeautifulSoup(response.text, \"html.parser\")\n",
        "    news_list = []\n",
        "\n",
        "    # 列表主選擇器\n",
        "    news_items = soup.select(\"li.press_edit_news_item, li.sa_item, div.cluster_item\")\n",
        "    MAX_ITEMS = 20\n",
        "    news_items = news_items[:MAX_ITEMS]\n",
        "\n",
        "    if not news_items:\n",
        "        return [], \"⚠ 警告：列表選擇器可能不匹配，無法抓取列表。請檢查網址是否為有效的 Naver 媒體或主題頁。\"\n",
        "\n",
        "    current_log = f\"💡 總共找到 {len(news_items)} 篇新聞，開始進行二次爬取...\\n\"\n",
        "\n",
        "    for n, item in enumerate(news_items, start=1):\n",
        "        # 抓取標題\n",
        "        title_tag = item.select_one(\"span.press_edit_news_title, a.sa_text_title\")\n",
        "        title = title_tag.get_text(strip=True) if title_tag else \"無標題\"\n",
        "\n",
        "        # 抓取文章連結\n",
        "        link_tag = item.select_one(\"a.press_edit_news_link, a.sa_text_title, a.n_tit\")\n",
        "        news_url = link_tag['href'] if link_tag and 'href' in link_tag.attrs else None\n",
        "\n",
        "        if not news_url or not news_url.startswith('https://n.news.naver.com'):\n",
        "             current_log += f\"[{n}/{MAX_ITEMS}] 跳過：連結無效或非 Naver 內頁 ({news_url})\\n\"\n",
        "             continue\n",
        "\n",
        "        # 呼叫二次爬蟲\n",
        "        date, author, content = 爬取_文章內文(news_url)\n",
        "\n",
        "        current_log += f\"[{n}/{MAX_ITEMS}] 處理：{title[:20]}... | 內文長度: {len(content)} 字符\\n\"\n",
        "\n",
        "        news_list.append({\n",
        "            \"標題\": title,\n",
        "            \"日期\": date,\n",
        "            \"作者\": author,\n",
        "            \"內文\": content,\n",
        "            \"連結\": news_url\n",
        "        })\n",
        "\n",
        "    return news_list, current_log"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 156,
      "metadata": {
        "id": "7FNixHMARr13"
      },
      "outputs": [],
      "source": [
        "def 分析新聞文本(texts, top_n=10):\n",
        "    tokenized_corpus = []\n",
        "    clean_pattern = re.compile(r'[0-9a-zA-Z\\t\\n\\r\\f\\v\\(\\)\\-\\+\\=\\[\\]\\{\\}\\<\\>@\\#\\$\\%\\^\\&\\*\\!\\~\\`\\?\\/\\\\]+')\n",
        "    for text in texts:\n",
        "        if pd.notna(text) and str(text).strip() not in ('無內文', ''):\n",
        "            cleaned_text = clean_pattern.sub('', str(text).strip())\n",
        "            if not cleaned_text: continue\n",
        "            korean_nouns = okt.nouns(cleaned_text)\n",
        "            stop_words_korean = ['기자', '뉴스', '연합', '제공', '따르', '위해', '통해', '이번', '그것', '이것', '저것', '에서', '에게', '오늘', '시간', '여러분']\n",
        "            filtered_words = [\n",
        "                word for word in korean_nouns\n",
        "                if len(word) > 1 and word.strip() and word not in stop_words_korean\n",
        "            ]\n",
        "            if not filtered_words: continue\n",
        "            tokenized_corpus.append(\" \".join(filtered_words))\n",
        "\n",
        "    if not tokenized_corpus:\n",
        "        return None, 0, \"\\n❌ 處理後的有效文本為空，請檢查文章內容。\"\n",
        "\n",
        "    # TF-IDF 計算\n",
        "    vectorizer = TfidfVectorizer(min_df=1, max_df=0.8)\n",
        "    tfidf_matrix = vectorizer.fit_transform(tokenized_corpus)\n",
        "    feature_names = vectorizer.get_feature_names_out()\n",
        "    sums = tfidf_matrix.sum(axis=0)\n",
        "\n",
        "    word_scores = []\n",
        "    for col, term in enumerate(feature_names):\n",
        "        word_scores.append((term, sums[0, col]))\n",
        "    word_scores.sort(key=lambda x: x[1], reverse=True)\n",
        "    top_words = word_scores[:top_n]\n",
        "\n",
        "    # Log 輸出格式化\n",
        "    log = \"\\n\" + \"=\" * 50 + \"\\n\"\n",
        "    log += f\"🔥 **【文章內文韓文熱詞分析結果】 (前 {len(top_words)} 名)** 🔥\\n\"\n",
        "    log += f\"💡 總共分析了 **{len(tokenized_corpus)}** 篇文章。\\n\"\n",
        "    log += \"-\" * 50 + \"\\n\"\n",
        "    for rank, (word, score) in enumerate(top_words, start=1):\n",
        "        log += f\"No. {rank}: **{word}** (總關鍵性分數: {score:.4f})\\n\"\n",
        "    log += \"=\" * 50 + \"\\n\"\n",
        "\n",
        "    # 返回熱詞列表、文章計數和 Log\n",
        "    return top_words, len(tokenized_corpus), log"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 157,
      "metadata": {
        "id": "P2eTqbllR7Kr"
      },
      "outputs": [],
      "source": [
        "def 使用_Gemini_API_熱詞分析(top_words_list):\n",
        "    # 函數內容同前\n",
        "    global GEMINI_CLIENT, GEMINI_MODEL, gemini_enabled\n",
        "\n",
        "    if not gemini_enabled:\n",
        "        return {\"洞察摘要\": \"Gemini 服務未啟用\", \"結論\": \"Gemini 服務未啟用\"}\n",
        "\n",
        "    if not top_words_list or len(top_words_list) == 0:\n",
        "        return {\"洞察摘要\": \"熱詞列表為空，無法分析\", \"結論\": \"熱詞列表為空，無法分析\"}\n",
        "\n",
        "    formatted_keywords = \"\\n\".join([f\"{rank+1}. {word} (分數: {score:.4f})\" for rank, (word, score) in enumerate(top_words_list)])\n",
        "\n",
        "    prompt = (\n",
        "        \"你是一位資深的數據分析師，請根據以下提供的 **20篇文章與熱詞分析結果 (韓文名詞與其關鍵性分數)**，提供專業的分析。\\n\\n\"\n",
        "        \"請根據這些熱詞，總結這20篇文章集的主要討論主題、趨勢或影響，並用**繁體中文**提供兩項分析結果：\\n\\n\"\n",
        "        \"1. **五句洞察摘要 (Insights)**：以條列式輸出 5 個客觀、精煉、且具洞察性的重點。請使用數字(如1、2、3...)作為開頭符號。\\n\\n\"\n",
        "        \"2. **一段結論 (Conclusion)**：用一段話總結這些熱詞所代表的整體趨勢和潛在影響，內容長度請嚴格控制在 110 到 130 個中文字。\\n\\n\"\n",
        "        \"請確保你的輸出格式如下：\\n\"\n",
        "        \"【洞察摘要】\\n\"\n",
        "        \"[第 1 句洞察]\\n\"\n",
        "        \"[第 2 句洞察]\\n\"\n",
        "        \"[第 3 句洞察]\\n\"\n",
        "        \"[第 4 句洞察]\\n\"\n",
        "        \"[第 5 句洞察]\\n\"\n",
        "        \"【結論】\\n\"\n",
        "        \"[一段 120 字左右的結論內容]\\n\\n\"\n",
        "        f\"熱詞分析結果 (前 {len(top_words_list)} 名)：\\n{formatted_keywords}\"\n",
        "    )\n",
        "\n",
        "    try:\n",
        "        response = GEMINI_CLIENT.models.generate_content(\n",
        "            model=GEMINI_MODEL,\n",
        "            contents=prompt\n",
        "        )\n",
        "\n",
        "        full_text = response.text.strip()\n",
        "\n",
        "        insight_match = full_text.find(\"【洞察摘要】\")\n",
        "        conclusion_match = full_text.find(\"【結論】\")\n",
        "\n",
        "        if insight_match != -1 and conclusion_match != -1:\n",
        "            insights_raw = full_text[insight_match + len(\"【洞察摘要】\"):conclusion_match].strip()\n",
        "            conclusion_raw = full_text[conclusion_match + len(\"【結論\") + 1:].strip()\n",
        "\n",
        "            insights_formatted = insights_raw.replace('\\n', '\\n- ').strip()\n",
        "            if not insights_formatted.startswith('- '):\n",
        "                insights_formatted = '- ' + insights_formatted\n",
        "\n",
        "            return {\n",
        "                \"洞察摘要\": insights_formatted,\n",
        "                \"結論\": conclusion_raw\n",
        "            }\n",
        "        else:\n",
        "             return {\"洞察摘要\": \"格式解析失敗，請檢查原始輸出\", \"結論\": full_text}\n",
        "\n",
        "    except Exception as e:\n",
        "        return {\"洞察摘要\": f\"Gemini 呼叫失敗: {str(e)}\", \"結論\": f\"Gemini 呼叫失敗: {str(e)}\"}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 158,
      "metadata": {
        "id": "MWZeKUziR-Jr"
      },
      "outputs": [],
      "source": [
        "def get_suffix_names(sheet_suffix):\n",
        "    # 根據後綴產生工作表名稱\n",
        "    suffix = f\" ({sheet_suffix})\" if sheet_suffix else \"\"\n",
        "    RESULT_SHEET_NAME = f\"爬蟲結果{suffix}\"\n",
        "    TOP_WORDS_SHEET_NAME = f\"熱詞分析結果{suffix}\"\n",
        "    AI_SUMMARY_SHEET_NAME = f\"AI總結{suffix}\"\n",
        "    return RESULT_SHEET_NAME, TOP_WORDS_SHEET_NAME, AI_SUMMARY_SHEET_NAME\n",
        "\n",
        "def run_only_scraper(url, sheet_suffix):\n",
        "    full_log = \"--- 認證狀態 ---\\n\" + sheet_auth_log + \"\\n\" + gemini_auth_log + \"\\n\" + \"=\" * 20 + \"\\n\"\n",
        "\n",
        "    if not url:\n",
        "        return full_log + \"\\n❌ 錯誤：請輸入有效的 Naver 新聞網址。\"\n",
        "\n",
        "    RESULT_SHEET_NAME, _, _ = get_suffix_names(sheet_suffix)\n",
        "\n",
        "    # 1. 爬蟲\n",
        "    full_log += \"--- 階段 1/2: 執行爬蟲 ---\\n\"\n",
        "    news_data, crawl_log = 爬取_naver_新聞(url)\n",
        "    full_log += crawl_log\n",
        "\n",
        "    if not news_data:\n",
        "        full_log += \"\\n❌ 爬蟲失敗：未抓取到任何有效新聞數據，流程中止。\"\n",
        "        return full_log\n",
        "\n",
        "    # 2. 寫入原始資料\n",
        "    full_log += \"\\n--- 階段 2/2: 寫入原始數據 ---\\n\"\n",
        "    write_data_log = 儲存_至_Google_Sheet(news_data, SHEET_KEY, sheet_name=RESULT_SHEET_NAME)\n",
        "    full_log += write_data_log\n",
        "    full_log += \"\\n\\n✅ **爬取與寫入 Sheet 流程完成！**\"\n",
        "\n",
        "    return full_log\n",
        "\n",
        "def run_tfidf_analysis(sheet_to_read, sheet_suffix):\n",
        "    full_log = \"--- 認證狀態 ---\\n\" + sheet_auth_log + \"\\n\" + \"=\" * 20 + \"\\n\"\n",
        "\n",
        "    if not sheet_to_read:\n",
        "        return full_log + \"\\n❌ 錯誤：請輸入要讀取的工作表名稱。\"\n",
        "\n",
        "    _, TOP_WORDS_SHEET_NAME, _ = get_suffix_names(sheet_suffix)\n",
        "\n",
        "    # 1. 讀取文章資料\n",
        "    df, read_log = 從_Google_Sheet_讀取資料(SHEET_KEY, sheet_to_read)\n",
        "    full_log += read_log\n",
        "\n",
        "    if df.empty:\n",
        "        return full_log\n",
        "\n",
        "    # 2. 準備分析數據\n",
        "    if '內文' in df.columns:\n",
        "        data_for_analysis = df['內文'].tolist()\n",
        "    elif 'Content' in df.columns:\n",
        "        data_for_analysis = df['Content'].tolist()\n",
        "    else:\n",
        "        full_log += \"\\n❌ 錯誤：在工作表 **'{sheet_to_read}'** 中找不到欄位 **'內文'** 或 **'Content'**，無法進行分析。\"\n",
        "        return full_log\n",
        "\n",
        "    # 3. TF-IDF 分析\n",
        "    top_words_list, analyzed_count, tfidf_log = 分析新聞文本(data_for_analysis, top_n=10)\n",
        "    full_log += tfidf_log\n",
        "\n",
        "    # 4. 寫入熱詞結果\n",
        "    if top_words_list and analyzed_count > 0:\n",
        "        write_words_log = 儲存_熱詞_至_Google_Sheet(top_words_list, SHEET_KEY, sheet_name=TOP_WORDS_SHEET_NAME)\n",
        "        full_log += \"\\n\" + \"=\" * 20 + \"\\n\" + write_words_log\n",
        "\n",
        "    return full_log\n",
        "\n",
        "\n",
        "def run_ai_hotword_analysis(sheet_to_read, sheet_suffix):\n",
        "    full_log = \"--- 認證狀態 ---\\n\" + sheet_auth_log + \"\\n\" + gemini_auth_log + \"\\n\" + \"=\" * 20 + \"\\n\"\n",
        "\n",
        "    if not gemini_enabled:\n",
        "        return full_log + \"\\n❌ 錯誤：Gemini API 未啟用，無法進行 AI 總結分析。\"\n",
        "\n",
        "    if not sheet_to_read:\n",
        "        return full_log + \"\\n❌ 錯誤：請輸入要讀取文章內容的工作表名稱。\"\n",
        "\n",
        "    _, _, AI_SUMMARY_SHEET_NAME = get_suffix_names(sheet_suffix)\n",
        "\n",
        "    # 1. 讀取文章資料\n",
        "    df, read_log = 從_Google_Sheet_讀取資料(SHEET_KEY, sheet_to_read)\n",
        "    full_log += read_log\n",
        "\n",
        "    if df.empty:\n",
        "        return full_log\n",
        "\n",
        "    # 2. 準備分析數據\n",
        "    if '內文' in df.columns:\n",
        "        data_for_analysis = df['內文'].tolist()\n",
        "    elif 'Content' in df.columns:\n",
        "        data_for_analysis = df['Content'].tolist()\n",
        "    else:\n",
        "        full_log += f\"\\n❌ 錯誤：在工作表 **'{sheet_to_read}'** 中找不到欄位 **'內文'** 或 **'Content'**，無法進行分析。\"\n",
        "        return full_log\n",
        "\n",
        "    # 3. TF-IDF 分析，取得熱詞\n",
        "    top_words_list, analyzed_count, tfidf_log = 分析新聞文本(data_for_analysis, top_n=10)\n",
        "    full_log += tfidf_log\n",
        "\n",
        "    if not top_words_list or analyzed_count == 0:\n",
        "        return full_log\n",
        "\n",
        "    # 4. 呼叫 Gemini API\n",
        "    full_log += \"\\n🧠 **執行熱詞總結分析 (Gemini API)...**\"\n",
        "    time.sleep(0.5) # 模擬思考時間\n",
        "    analysis_result = 使用_Gemini_API_熱詞分析(top_words_list)\n",
        "\n",
        "    # 5. 寫入 AI 總結結果\n",
        "    write_summary_log = 儲存_分析結果_至_Google_Sheet(analysis_result, SHEET_KEY, sheet_name=AI_SUMMARY_SHEET_NAME)\n",
        "    full_log += \"\\n\" + \"=\" * 20 + \"\\n\" + write_summary_log\n",
        "\n",
        "    # 6. 整理 AI 總結輸出\n",
        "    ai_output = (\n",
        "        \"--- Gemini AI 總結分析結果 ---\\n\\n\"\n",
        "        f\"**Gemini 洞察摘要：**\\n{analysis_result['洞察摘要']}\\n\\n\"\n",
        "        f\"**Gemini 結論：**\\n{analysis_result['結論']}\"\n",
        "    )\n",
        "\n",
        "    return full_log, ai_output"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def run_all_in_one(url, sheet_suffix):\n",
        "    \"\"\"分頁 4：爬蟲、TF-IDF、AI 總結一次完成\"\"\"\n",
        "\n",
        "    full_log = \"--- 認證狀態 ---\\n\" + sheet_auth_log + \"\\n\" + gemini_auth_log + \"\\n\" + \"=\" * 20 + \"\\n\"\n",
        "    ai_output = \"分析尚未執行。\"\n",
        "\n",
        "    if not url:\n",
        "        return full_log + \"\\n❌ 錯誤：請輸入有效的 Naver 新聞網址。\", ai_output\n",
        "\n",
        "    if not gemini_enabled:\n",
        "        return full_log + \"\\n❌ 錯誤：Gemini API 未啟用，無法進行 AI 總結分析。\", ai_output\n",
        "\n",
        "    RESULT_SHEET_NAME, TOP_WORDS_SHEET_NAME, AI_SUMMARY_SHEET_NAME = get_suffix_names(sheet_suffix)\n",
        "\n",
        "    # 1. 爬蟲\n",
        "    full_log += \"--- 階段 1/4: 執行爬蟲 ---\\n\"\n",
        "    news_data, crawl_log = 爬取_naver_新聞(url)\n",
        "    full_log += crawl_log\n",
        "\n",
        "    if not news_data:\n",
        "        return full_log, ai_output\n",
        "\n",
        "    # 2. 寫入原始資料\n",
        "    full_log += \"\\n--- 階段 2/4: 寫入原始數據 ---\\n\"\n",
        "    write_data_log = 儲存_至_Google_Sheet(news_data, SHEET_KEY, sheet_name=RESULT_SHEET_NAME)\n",
        "    full_log += write_data_log\n",
        "\n",
        "    # 3. TF-IDF 分析\n",
        "    full_log += \"\\n--- 階段 3/4: TF-IDF 熱詞分析 ---\\n\"\n",
        "    data_for_analysis = [item['內文'] for item in news_data]\n",
        "    top_words_list, analyzed_count, tfidf_log = 分析新聞文本(data_for_analysis, top_n=10)\n",
        "    full_log += tfidf_log\n",
        "\n",
        "    if not top_words_list or analyzed_count == 0:\n",
        "        return full_log + \"\\n❌ 由於無有效內文，AI 總結中止。\", ai_output\n",
        "\n",
        "    # 寫入熱詞結果\n",
        "    write_words_log = 儲存_熱詞_至_Google_Sheet(top_words_list, SHEET_KEY, sheet_name=TOP_WORDS_SHEET_NAME)\n",
        "    full_log += \"\\n\" + write_words_log\n",
        "\n",
        "\n",
        "    # 4. 呼叫 Gemini API\n",
        "    full_log += \"\\n--- 階段 4/4: Gemini AI 總結 ---\\n\"\n",
        "    full_log += \"\\n🧠 **執行熱詞總結分析 (Gemini API)...**\"\n",
        "    time.sleep(0.5) # 模擬思考時間\n",
        "    analysis_result = 使用_Gemini_API_熱詞分析(top_words_list)\n",
        "\n",
        "    # 寫入 AI 總結結果\n",
        "    write_summary_log = 儲存_分析結果_至_Google_Sheet(analysis_result, SHEET_KEY, sheet_name=AI_SUMMARY_SHEET_NAME)\n",
        "    full_log += \"\\n\" + write_summary_log\n",
        "\n",
        "    # 整理 AI 總結輸出\n",
        "    ai_output = (\n",
        "        \"--- Gemini AI 總結分析結果 ---\\n\\n\"\n",
        "        f\"**Gemini 洞察摘要：**\\n{analysis_result['洞察摘要']}\\n\\n\"\n",
        "        f\"**Gemini 結論：**\\n{analysis_result['結論']}\"\n",
        "    )\n",
        "    full_log += \"\\n\\n✅ **所有步驟執行完成！** 結果已寫入 Sheet。\"\n",
        "\n",
        "    return full_log, ai_output"
      ],
      "metadata": {
        "id": "YmeWJWTs-FHl"
      },
      "execution_count": 159,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 166,
      "metadata": {
        "id": "H9OMdCRcSEHq"
      },
      "outputs": [],
      "source": [
        "with gr.Blocks(title=\"Naver 新聞分析工具\") as demo:\n",
        "    gr.Markdown(\"# Naver 新聞分析工具\")\n",
        "    gr.Markdown(\"### 請前往 [Naver 新聞主頁](https://news.naver.com/) 尋找網址。也可點進[Google Sheet](https://docs.google.com/spreadsheets/d/1P4V-D8o7bXHHHVMRQxG6voDjeyVUcDRB_PAahWajUUM/edit?usp=sharing)查看資料\")\n",
        "\n",
        "    # 共同輸入區塊\n",
        "    gr.Markdown(\"### 共同輸入區\")\n",
        "    with gr.Row():\n",
        "        sheet_suffix_input = gr.Textbox(\n",
        "            label=\"自訂工作表名稱後綴\",\n",
        "            placeholder=\"例如：MBC新聞。將用於區分所有輸出的工作表名稱。\",\n",
        "            scale=3\n",
        "        )\n",
        "        gr.Markdown(\"---\")\n",
        "\n",
        "    with gr.Accordion(\"工作表刪除\", open=False):\n",
        "        delete_btn = gr.Button(\"🚨 刪除所有相關工作表 (使用上方後綴)\")\n",
        "        delete_output = gr.Textbox(\n",
        "            label=\"刪除三張工作表\",\n",
        "            lines=5,\n",
        "            interactive=False\n",
        "        )\n",
        "\n",
        "        delete_btn.click(\n",
        "            fn=delete_sheets_by_suffix,\n",
        "            inputs=[sheet_suffix_input],\n",
        "            outputs=delete_output\n",
        "        )\n",
        "\n",
        "    # 功能區塊\n",
        "    gr.Markdown(\"### 功能區\")\n",
        "\n",
        "    with gr.Tabs():\n",
        "\n",
        "        # 1. All in one (Option 1)\n",
        "        with gr.TabItem(\"爬蟲/熱詞分析/AI總結\"):\n",
        "            gr.Markdown(\"### 一鍵執行爬蟲、熱詞分析、AI 總結 (快速獲取結果)\")\n",
        "\n",
        "            url_input_1 = gr.Textbox(\n",
        "                label=\"Naver 新聞媒體/主題網址\",\n",
        "                placeholder=\"例如：https://media.naver.com/press/658?sid=102\"\n",
        "            )\n",
        "\n",
        "            all_in_one_btn = gr.Button(\"🔥 開始【一鍵完整分析】並寫入 Sheet (請耐心等待)\")\n",
        "\n",
        "\n",
        "            with gr.Row():\n",
        "\n",
        "                all_in_one_log = gr.Textbox(\n",
        "                    label=\"執行爬蟲與熱詞分析\",\n",
        "                    lines=20,\n",
        "                    interactive=False\n",
        "                )\n",
        "\n",
        "                all_in_one_summary = gr.Textbox(\n",
        "                    label=\"Gemini AI 總結結果\",\n",
        "                    lines=20,\n",
        "                    interactive=False\n",
        "                )\n",
        "\n",
        "            all_in_one_btn.click(\n",
        "                fn=run_all_in_one,\n",
        "                inputs=[url_input_1, sheet_suffix_input],\n",
        "                outputs=[all_in_one_log, all_in_one_summary]\n",
        "            )\n",
        "\n",
        "        # 2. 爬取 Naver 新聞 (Option 2)\n",
        "        with gr.TabItem(\"爬取 Naver 新聞\"):\n",
        "            gr.Markdown(\"### 爬取 Naver 新聞 (執行爬蟲)\")\n",
        "\n",
        "            url_input_2 = gr.Textbox(\n",
        "                label=\"Naver 新聞媒體/主題網址\",\n",
        "                placeholder=\"例如：https://media.naver.com/press/658?sid=102\"\n",
        "            )\n",
        "\n",
        "            scrape_btn = gr.Button(\"🚀 開始爬取並寫入 Sheet (爬蟲結果)\")\n",
        "\n",
        "            scrape_output = gr.Textbox(\n",
        "                label=\"文章爬取\",\n",
        "                lines=20,\n",
        "                interactive=False\n",
        "            )\n",
        "\n",
        "            scrape_btn.click(\n",
        "                fn=run_only_scraper,\n",
        "                inputs=[url_input_2, sheet_suffix_input],\n",
        "                outputs=scrape_output\n",
        "            )\n",
        "\n",
        "        # 3. 讀取 Google Sheet 資料 (Option 3)\n",
        "        with gr.TabItem(\"讀取 Sheet 資料\"):\n",
        "            gr.Markdown(\"### 讀取 Google Sheet 資料 (僅執行 TF-IDF 熱詞分析)\")\n",
        "\n",
        "            read_tfidf_sheet_input = gr.Textbox(\n",
        "                label=\"要讀取的工作表名稱(限爬蟲結果)\",\n",
        "                placeholder=\"例如：爬蟲結果, 爬蟲結果 (MBC新聞)\"\n",
        "            )\n",
        "\n",
        "            tfidf_btn = gr.Button(\"📊 開始讀取、分析並寫入 Sheet (熱詞分析結果)\")\n",
        "\n",
        "            tfidf_output = gr.Textbox(\n",
        "                label=\"Sheet讀取並分析熱詞\",\n",
        "                lines=20,\n",
        "                interactive=False\n",
        "            )\n",
        "\n",
        "            tfidf_btn.click(\n",
        "                fn=run_tfidf_analysis,\n",
        "                inputs=[read_tfidf_sheet_input, sheet_suffix_input],\n",
        "                outputs=tfidf_output\n",
        "            )\n",
        "\n",
        "        # 4. AI 分析 (Option 4)\n",
        "        with gr.TabItem(\"AI 分析 (熱詞總結)\"):\n",
        "            gr.Markdown(\"### AI 分析 (針對爬取文章的熱詞進行總結)\")\n",
        "\n",
        "            ai_sheet_input = gr.Textbox(\n",
        "                label=\"要讀取的工作表名稱(限爬蟲結果)\",\n",
        "                placeholder=\"例如：爬蟲結果, 爬蟲結果 (MBC新聞)\"\n",
        "            )\n",
        "\n",
        "            ai_btn = gr.Button(\"🧠 開始熱詞總結分析並寫入 Sheet (AI總結)\")\n",
        "\n",
        "            ai_output_log = gr.Textbox(\n",
        "                label=\"分析爬取內容與熱詞\",\n",
        "                lines=10,\n",
        "                interactive=False\n",
        "            )\n",
        "\n",
        "            ai_output_summary = gr.Textbox(\n",
        "                label=\"Gemini AI 總結結果\",\n",
        "                lines=10,\n",
        "                interactive=False\n",
        "            )\n",
        "\n",
        "            ai_btn.click(\n",
        "                fn=run_ai_hotword_analysis,\n",
        "                inputs=[ai_sheet_input, sheet_suffix_input],\n",
        "                outputs=[ai_output_log, ai_output_summary]\n",
        "            )\n",
        "\n",
        "\n",
        "        # 5. 工作表匯出 (Option 5)\n",
        "        with gr.TabItem(\"工作表匯出\"):\n",
        "            gr.Markdown(\"### 匯出指定工作表內容並提供下載\")\n",
        "\n",
        "            with gr.Accordion(\"📦 單一工作表匯出\", open=False):\n",
        "              export_sheet_input = gr.Textbox(\n",
        "                label=\"要匯出資料的單一工作表名稱\",\n",
        "                placeholder=\"例如：爬蟲結果 (MBC新聞), 熱詞分析結果 (MBC新聞)。工作表名稱可在Google Sheet中查看\"\n",
        "              )\n",
        "\n",
        "              format_radio_single = gr.Radio(\n",
        "                [\"CSV\", \"JSON\"],\n",
        "                label=\"選擇匯出格式\",\n",
        "                value=\"CSV\"\n",
        "              )\n",
        "\n",
        "              export_btn_single = gr.Button(\"⬇️ 讀取並產生下載檔案 (單一工作表)\")\n",
        "\n",
        "              with gr.Row():\n",
        "                export_file_output_single = gr.File(\n",
        "                    label=\"下載檔案\",\n",
        "                    scale=1\n",
        "                )\n",
        "                export_log_output_single = gr.Textbox(\n",
        "                    label=\"匯出檔案\",\n",
        "                    lines=8,\n",
        "                    interactive=False,\n",
        "                    scale=2\n",
        "                )\n",
        "\n",
        "              export_btn_single.click(\n",
        "                fn=run_and_clear_single_export,\n",
        "                inputs=[export_sheet_input, format_radio_single],\n",
        "                outputs=[export_file_output_single, export_log_output_single, export_sheet_input]\n",
        "                )\n",
        "\n",
        "            gr.Markdown(\"---\")\n",
        "\n",
        "        # --- 區塊 5.2: 一鍵匯出所有工作表 ---\n",
        "            with gr.Accordion(\"🗜️ ZIP 壓縮包匯出 (使用後綴)\", open=False):\n",
        "              gr.Markdown(\"此功能將使用上方**工作表命名**的 `工作表名稱後綴`，將 `爬蟲結果`、`熱詞分析結果`、`AI總結` 三個工作表壓縮成一個 ZIP 檔案。\")\n",
        "\n",
        "              format_radio_all = gr.Radio(\n",
        "                [\"CSV\", \"JSON\"],\n",
        "                label=\"選擇壓縮包內部的檔案格式\",\n",
        "                value=\"CSV\"\n",
        "              )\n",
        "\n",
        "              export_btn_all = gr.Button(\"📦 開始【一鍵匯出】所有相關工作表 (ZIP)\")\n",
        "\n",
        "              with gr.Row():\n",
        "                export_file_output_all = gr.File(\n",
        "                    label=\"下載檔案 (ZIP檔)\",\n",
        "                    scale=1\n",
        "                )\n",
        "                export_log_output_all = gr.Textbox(\n",
        "                    label=\"匯出檔案\",\n",
        "                    lines=8,\n",
        "                    interactive=False,\n",
        "                    scale=2\n",
        "                )\n",
        "\n",
        "              export_btn_all.click(\n",
        "                fn=export_all_by_suffix,\n",
        "                inputs=[sheet_suffix_input, format_radio_all],\n",
        "                outputs=[export_file_output_all, export_log_output_all]\n",
        "                )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 167,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 680
        },
        "id": "osvWV8bOSG04",
        "outputId": "c4c56397-c7d3-435f-d777-3f593872b4bc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "It looks like you are running Gradio on a hosted Jupyter notebook, which requires `share=True`. Automatically setting `share=True` (you can turn this off by setting `share=False` in `launch()` explicitly).\n",
            "\n",
            "Colab notebook detected. This cell will run indefinitely so that you can see errors and logs. To turn off, set debug=False in launch().\n",
            "* Running on public URL: https://8e0f4cbb67025983f0.gradio.live\n",
            "\n",
            "This share link expires in 1 week. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://8e0f4cbb67025983f0.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Keyboard interruption in main thread... closing server.\n",
            "Killing tunnel 127.0.0.1:7860 <> https://8e0f4cbb67025983f0.gradio.live\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 167
        }
      ],
      "source": [
        "demo.launch(debug=True)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyObOcnkkg2BZExJRG3P/OZ0",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}